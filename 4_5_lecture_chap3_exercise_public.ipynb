{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ffalo64/rabbit_challenge/blob/main/4_5_lecture_chap3_exercise_public.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "LnVoFWGNgpzX"
      },
      "cell_type": "markdown",
      "source": [
        "必要ファイルをダウンロード\n"
      ]
    },
    {
      "metadata": {
        "id": "2lNAJ9AXgmuf",
        "outputId": "b5dd6f6a-94ff-4233-e401-928bce0c49cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "! wget https://www.dropbox.com/s/m60lb68dg6k2im1/config.ini\n",
        "! mkdir model\n",
        "! mkdir model/ntt_output\n",
        "! mkdir data\n",
        "! mkdir data/ntt\n",
        "! wget https://www.dropbox.com/s/t5s09w69mruqr8v/dev.tsv -P data/ntt/\n",
        "! wget https://www.dropbox.com/s/ny2kxiotq0bgqr1/eval.tsv -P data/ntt/\n",
        "! wget https://www.dropbox.com/s/h24zlpk15g1k9sv/train.tsv -P data/ntt/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-04 23:06:40--  https://www.dropbox.com/s/m60lb68dg6k2im1/config.ini\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/m60lb68dg6k2im1/config.ini [following]\n",
            "--2023-05-04 23:06:40--  https://www.dropbox.com/s/raw/m60lb68dg6k2im1/config.ini\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc8a342303f429a8d914cddecfa9.dl.dropboxusercontent.com/cd/0/inline/B7eYodzhBDysJnW_Z1x4_QgiBpMfx6G0U04-XFbGj1Z6lLlB1jqz190Q6-GPruF-w-LL3Ylvrvg9yIvG5eGi_OhTv7W_emKEPtU06DVfmxjQPeYjf8YKBtpUIDzQZcyXBVh-hAv4ypd5HViEygiJvrTYx-xaONLCcBkIUujeM3ynFQ/file# [following]\n",
            "--2023-05-04 23:06:40--  https://uc8a342303f429a8d914cddecfa9.dl.dropboxusercontent.com/cd/0/inline/B7eYodzhBDysJnW_Z1x4_QgiBpMfx6G0U04-XFbGj1Z6lLlB1jqz190Q6-GPruF-w-LL3Ylvrvg9yIvG5eGi_OhTv7W_emKEPtU06DVfmxjQPeYjf8YKBtpUIDzQZcyXBVh-hAv4ypd5HViEygiJvrTYx-xaONLCcBkIUujeM3ynFQ/file\n",
            "Resolving uc8a342303f429a8d914cddecfa9.dl.dropboxusercontent.com (uc8a342303f429a8d914cddecfa9.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uc8a342303f429a8d914cddecfa9.dl.dropboxusercontent.com (uc8a342303f429a8d914cddecfa9.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1077 (1.1K) [text/plain]\n",
            "Saving to: ‘config.ini.1’\n",
            "\n",
            "config.ini.1        100%[===================>]   1.05K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-04 23:06:41 (102 MB/s) - ‘config.ini.1’ saved [1077/1077]\n",
            "\n",
            "mkdir: cannot create directory ‘model’: File exists\n",
            "mkdir: cannot create directory ‘model/ntt_output’: File exists\n",
            "mkdir: cannot create directory ‘data’: File exists\n",
            "mkdir: cannot create directory ‘data/ntt’: File exists\n",
            "--2023-05-04 23:06:41--  https://www.dropbox.com/s/t5s09w69mruqr8v/dev.tsv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/t5s09w69mruqr8v/dev.tsv [following]\n",
            "--2023-05-04 23:06:42--  https://www.dropbox.com/s/raw/t5s09w69mruqr8v/dev.tsv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucbe4bd1fac7ad77f87e7d8d09ca.dl.dropboxusercontent.com/cd/0/inline/B7eNAn1qNVaqDoLwnPSWpjNsdXjZPtL_HJNoKxup76vhm6HrooQVy_EiBEaiGwwZhAdjhs80Hg765MlRT4LAtyNsUOjClsmKhSIhUIEQp-Lp4rY7dLJ2LvKoRZlBG_Cdbfx9KbTl_P4scw0ErqRqvC8SzjkWyTfNvP6Y16D9n5Oz_A/file# [following]\n",
            "--2023-05-04 23:06:42--  https://ucbe4bd1fac7ad77f87e7d8d09ca.dl.dropboxusercontent.com/cd/0/inline/B7eNAn1qNVaqDoLwnPSWpjNsdXjZPtL_HJNoKxup76vhm6HrooQVy_EiBEaiGwwZhAdjhs80Hg765MlRT4LAtyNsUOjClsmKhSIhUIEQp-Lp4rY7dLJ2LvKoRZlBG_Cdbfx9KbTl_P4scw0ErqRqvC8SzjkWyTfNvP6Y16D9n5Oz_A/file\n",
            "Resolving ucbe4bd1fac7ad77f87e7d8d09ca.dl.dropboxusercontent.com (ucbe4bd1fac7ad77f87e7d8d09ca.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to ucbe4bd1fac7ad77f87e7d8d09ca.dl.dropboxusercontent.com (ucbe4bd1fac7ad77f87e7d8d09ca.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 222195 (217K) [text/plain]\n",
            "Saving to: ‘data/ntt/dev.tsv.1’\n",
            "\n",
            "dev.tsv.1           100%[===================>] 216.99K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-05-04 23:06:42 (5.81 MB/s) - ‘data/ntt/dev.tsv.1’ saved [222195/222195]\n",
            "\n",
            "--2023-05-04 23:06:42--  https://www.dropbox.com/s/ny2kxiotq0bgqr1/eval.tsv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/ny2kxiotq0bgqr1/eval.tsv [following]\n",
            "--2023-05-04 23:06:42--  https://www.dropbox.com/s/raw/ny2kxiotq0bgqr1/eval.tsv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb9805c8dee537a16c3c13f2123.dl.dropboxusercontent.com/cd/0/inline/B7ex-S-dJvGOlCY8-zp_1168U3zbss6WEwY6hBGPL6T35JURAi0TbrHpDMX-kAyNaAf9dfIKYI-TjEGDvsne2Mq_XK_utpH2595VzqAXsxyZcWbAy8-1yUa5SMS6x9AAcX5k1Ex-KHcbQBnreKw1qCVxAXvjWbZ41WLwK_cOwhMqvA/file# [following]\n",
            "--2023-05-04 23:06:43--  https://ucb9805c8dee537a16c3c13f2123.dl.dropboxusercontent.com/cd/0/inline/B7ex-S-dJvGOlCY8-zp_1168U3zbss6WEwY6hBGPL6T35JURAi0TbrHpDMX-kAyNaAf9dfIKYI-TjEGDvsne2Mq_XK_utpH2595VzqAXsxyZcWbAy8-1yUa5SMS6x9AAcX5k1Ex-KHcbQBnreKw1qCVxAXvjWbZ41WLwK_cOwhMqvA/file\n",
            "Resolving ucb9805c8dee537a16c3c13f2123.dl.dropboxusercontent.com (ucb9805c8dee537a16c3c13f2123.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to ucb9805c8dee537a16c3c13f2123.dl.dropboxusercontent.com (ucb9805c8dee537a16c3c13f2123.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 235514 (230K) [text/plain]\n",
            "Saving to: ‘data/ntt/eval.tsv.1’\n",
            "\n",
            "eval.tsv.1          100%[===================>] 229.99K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-05-04 23:06:43 (5.26 MB/s) - ‘data/ntt/eval.tsv.1’ saved [235514/235514]\n",
            "\n",
            "--2023-05-04 23:06:43--  https://www.dropbox.com/s/h24zlpk15g1k9sv/train.tsv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/h24zlpk15g1k9sv/train.tsv [following]\n",
            "--2023-05-04 23:06:43--  https://www.dropbox.com/s/raw/h24zlpk15g1k9sv/train.tsv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc2324ac902e008b1eb016d8ab72.dl.dropboxusercontent.com/cd/0/inline/B7f21bMTXNgRB7AwTceB4L0yEQVlvssDPjcVYjbVpSuNcF4Ji3zIPvCnTYlFiU9bPenBOLQvbCxHqQT0DEPBFq60H0DQ9mcL3a-YNJjlrLFqEmgaofATJtaExjmBIXvefDPjtjYwAfgRxZiddB-_F7gPa0J5-8CA4uh_6myx7zZkmg/file# [following]\n",
            "--2023-05-04 23:06:44--  https://uc2324ac902e008b1eb016d8ab72.dl.dropboxusercontent.com/cd/0/inline/B7f21bMTXNgRB7AwTceB4L0yEQVlvssDPjcVYjbVpSuNcF4Ji3zIPvCnTYlFiU9bPenBOLQvbCxHqQT0DEPBFq60H0DQ9mcL3a-YNJjlrLFqEmgaofATJtaExjmBIXvefDPjtjYwAfgRxZiddB-_F7gPa0J5-8CA4uh_6myx7zZkmg/file\n",
            "Resolving uc2324ac902e008b1eb016d8ab72.dl.dropboxusercontent.com (uc2324ac902e008b1eb016d8ab72.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uc2324ac902e008b1eb016d8ab72.dl.dropboxusercontent.com (uc2324ac902e008b1eb016d8ab72.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1998277 (1.9M) [text/plain]\n",
            "Saving to: ‘data/ntt/train.tsv.1’\n",
            "\n",
            "train.tsv.1         100%[===================>]   1.91M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-05-04 23:06:44 (20.7 MB/s) - ‘data/ntt/train.tsv.1’ saved [1998277/1998277]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "tap-nTRBPIGg",
        "outputId": "51faf054-0b90-42e9-9066-58b6cad5f56a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "! wget https://www.dropbox.com/s/8bdr3wvlufuqj8m/model.ckpt-1000000.index -P model/\n",
        "! wget https://www.dropbox.com/s/iuinyuzrvnkrmp2/model.ckpt-1000000.meta -P model/\n",
        "! wget https://www.dropbox.com/s/01evq5zs9hbspyj/wiki-ja.model -P model/\n",
        "! wget https://www.dropbox.com/s/b464zufwp05pwz4/wiki-ja.vocab -P model/\n",
        "! wget https://www.dropbox.com/s/bj6itc7edybfq6x/model.ckpt-1000000.data-00000-of-00001 -P model/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-04 23:06:48--  https://www.dropbox.com/s/8bdr3wvlufuqj8m/model.ckpt-1000000.index\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/8bdr3wvlufuqj8m/model.ckpt-1000000.index [following]\n",
            "--2023-05-04 23:06:48--  https://www.dropbox.com/s/raw/8bdr3wvlufuqj8m/model.ckpt-1000000.index\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc211bd39738ba02658cb908a521.dl.dropboxusercontent.com/cd/0/inline/B7c1hDtwnT9JBCU89pKHPlfOpwyar7gep6ANdlG6wnZBCbuUxwmEmREQrKrx1zpa_zjyaBYFmkBU2gUE2OYa6U_amYGDj3g_Xeu8mpNQJmZsJ3nTc4p-oqzWdTwe9OwUlR9ezifsv1cWFP_asXdKXhFonTmJcHPtphCzk5RL6yFHAA/file# [following]\n",
            "--2023-05-04 23:06:48--  https://uc211bd39738ba02658cb908a521.dl.dropboxusercontent.com/cd/0/inline/B7c1hDtwnT9JBCU89pKHPlfOpwyar7gep6ANdlG6wnZBCbuUxwmEmREQrKrx1zpa_zjyaBYFmkBU2gUE2OYa6U_amYGDj3g_Xeu8mpNQJmZsJ3nTc4p-oqzWdTwe9OwUlR9ezifsv1cWFP_asXdKXhFonTmJcHPtphCzk5RL6yFHAA/file\n",
            "Resolving uc211bd39738ba02658cb908a521.dl.dropboxusercontent.com (uc211bd39738ba02658cb908a521.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc211bd39738ba02658cb908a521.dl.dropboxusercontent.com (uc211bd39738ba02658cb908a521.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/B7e3sKkpEieIOD5do_WvW057VezcdmfuBpr74N1AOOjMCSTemQzYzJzr9TRjzzfEmeIYcRYMAQlU0hG5ZS1OnPvvlOjgyS3jpmalbhZgGB281gV9m__2e4XMwPrGSACmF5N18pWwdGn6R9MsVh-cwipPgkW89oCyxVyVnCI99DFaTZ5xrMuCANiKSnclPGFCNpfxLyE9XtrDllZjCbgl_J5U-z6_8KYFP5kcSVHQGaM5sF3atMweZDi5bgonrtv8TcgspddyzAMScs5Yqym5kgCx-cE-JGz_wDla7EvvT5oaIZU--zuoyCBpcxct2pVxsZ9eSBCPOQflNv9TSFxr9ufNFc86SZrdRae9lEm0NToOwl--vHA9c6rMxIXhsqZr6Y2IeDz6tgnDTE3LVVvGYwEQhxYnsYnHbsiVBKm1ZC1Smg/file [following]\n",
            "--2023-05-04 23:06:49--  https://uc211bd39738ba02658cb908a521.dl.dropboxusercontent.com/cd/0/inline2/B7e3sKkpEieIOD5do_WvW057VezcdmfuBpr74N1AOOjMCSTemQzYzJzr9TRjzzfEmeIYcRYMAQlU0hG5ZS1OnPvvlOjgyS3jpmalbhZgGB281gV9m__2e4XMwPrGSACmF5N18pWwdGn6R9MsVh-cwipPgkW89oCyxVyVnCI99DFaTZ5xrMuCANiKSnclPGFCNpfxLyE9XtrDllZjCbgl_J5U-z6_8KYFP5kcSVHQGaM5sF3atMweZDi5bgonrtv8TcgspddyzAMScs5Yqym5kgCx-cE-JGz_wDla7EvvT5oaIZU--zuoyCBpcxct2pVxsZ9eSBCPOQflNv9TSFxr9ufNFc86SZrdRae9lEm0NToOwl--vHA9c6rMxIXhsqZr6Y2IeDz6tgnDTE3LVVvGYwEQhxYnsYnHbsiVBKm1ZC1Smg/file\n",
            "Reusing existing connection to uc211bd39738ba02658cb908a521.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23350 (23K) [application/octet-stream]\n",
            "Saving to: ‘model/model.ckpt-1000000.index.1’\n",
            "\n",
            "model.ckpt-1000000. 100%[===================>]  22.80K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-05-04 23:06:49 (12.1 MB/s) - ‘model/model.ckpt-1000000.index.1’ saved [23350/23350]\n",
            "\n",
            "--2023-05-04 23:06:49--  https://www.dropbox.com/s/iuinyuzrvnkrmp2/model.ckpt-1000000.meta\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/iuinyuzrvnkrmp2/model.ckpt-1000000.meta [following]\n",
            "--2023-05-04 23:06:49--  https://www.dropbox.com/s/raw/iuinyuzrvnkrmp2/model.ckpt-1000000.meta\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucdf848a9635c46283e8b2a2f8fc.dl.dropboxusercontent.com/cd/0/inline/B7e2jNtiBc6JXBonPOLgmsMw2j4pOJ9MIt_eU2Y23AcDcKoLw382EQN5AWKK8EpIgH2Zin3_gaPhoqHBqgLvo9UaUJgD5jNP6RE2LuK4x4rMovHeLnaM3wEaQdSZwZrqG_lkPRB361iDHMgIePneNNgb-_9TEQfGW-1-a1y4ETkBFQ/file# [following]\n",
            "--2023-05-04 23:06:49--  https://ucdf848a9635c46283e8b2a2f8fc.dl.dropboxusercontent.com/cd/0/inline/B7e2jNtiBc6JXBonPOLgmsMw2j4pOJ9MIt_eU2Y23AcDcKoLw382EQN5AWKK8EpIgH2Zin3_gaPhoqHBqgLvo9UaUJgD5jNP6RE2LuK4x4rMovHeLnaM3wEaQdSZwZrqG_lkPRB361iDHMgIePneNNgb-_9TEQfGW-1-a1y4ETkBFQ/file\n",
            "Resolving ucdf848a9635c46283e8b2a2f8fc.dl.dropboxusercontent.com (ucdf848a9635c46283e8b2a2f8fc.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to ucdf848a9635c46283e8b2a2f8fc.dl.dropboxusercontent.com (ucdf848a9635c46283e8b2a2f8fc.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3938703 (3.8M) [text/plain]\n",
            "Saving to: ‘model/model.ckpt-1000000.meta.1’\n",
            "\n",
            "model.ckpt-1000000. 100%[===================>]   3.76M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-05-04 23:06:50 (43.9 MB/s) - ‘model/model.ckpt-1000000.meta.1’ saved [3938703/3938703]\n",
            "\n",
            "--2023-05-04 23:06:50--  https://www.dropbox.com/s/01evq5zs9hbspyj/wiki-ja.model\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/01evq5zs9hbspyj/wiki-ja.model [following]\n",
            "--2023-05-04 23:06:50--  https://www.dropbox.com/s/raw/01evq5zs9hbspyj/wiki-ja.model\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb75219c897d6d1b46c734915b3.dl.dropboxusercontent.com/cd/0/inline/B7c9OF1bRbGm3dONmxuq-xSnahL3nclvj_JQ3bB4JdSRlIJkdb1mD7n3oEbheQOq61GB2UBUvk5h8wMOadNWOvarjGmYUKnqNpEvHxXZeLzTW4dQkkvNC_LQIjQkfwG9UoT7h1vDHmPjCSwnnzIrQTsLmHzv1mbdMOE6hEBOMWwoJw/file# [following]\n",
            "--2023-05-04 23:06:50--  https://ucb75219c897d6d1b46c734915b3.dl.dropboxusercontent.com/cd/0/inline/B7c9OF1bRbGm3dONmxuq-xSnahL3nclvj_JQ3bB4JdSRlIJkdb1mD7n3oEbheQOq61GB2UBUvk5h8wMOadNWOvarjGmYUKnqNpEvHxXZeLzTW4dQkkvNC_LQIjQkfwG9UoT7h1vDHmPjCSwnnzIrQTsLmHzv1mbdMOE6hEBOMWwoJw/file\n",
            "Resolving ucb75219c897d6d1b46c734915b3.dl.dropboxusercontent.com (ucb75219c897d6d1b46c734915b3.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to ucb75219c897d6d1b46c734915b3.dl.dropboxusercontent.com (ucb75219c897d6d1b46c734915b3.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/B7fD6lcjZ3rgZsIlMYtA6FFxKfDnVaJ0iIH6e7KxBuAwAeEkkhimSXaRObSvPQX_KRzLrnGiaV0p4VvsQvAOcQ5YsGOlRvOIIu3tmQ3Ji-Iy6KDDXYLGxtL4ZGNHjkOOZHm0J_U3JLoDnM3bjKUY4OIgsCbdwwPZ4_qpcUv5YyvzSlvldZ9CHZ-JRoAcoeVX9yb18Wt6Oss46Oz9bLN7Z3Zx2ZBNzw0K3eQnVV0IHmQeRZfW2rKl0pGkl3kzRNISZa3NR1QfP-jyvk6i5SIx3kqB6zlEZDeWqkP6bG_mFoU6I44G1AvEv1TnpS_oKTT0mOGjj7hshRwa4AORp8AnUalUtGnb2VhI-KaDU0x2vNqeezbOBllTKF4ZLftPaAEAWZZ9T-fbJLjjXJc_YmfDpItAV14x9dtGyhHk77BLkHUoCw/file [following]\n",
            "--2023-05-04 23:06:51--  https://ucb75219c897d6d1b46c734915b3.dl.dropboxusercontent.com/cd/0/inline2/B7fD6lcjZ3rgZsIlMYtA6FFxKfDnVaJ0iIH6e7KxBuAwAeEkkhimSXaRObSvPQX_KRzLrnGiaV0p4VvsQvAOcQ5YsGOlRvOIIu3tmQ3Ji-Iy6KDDXYLGxtL4ZGNHjkOOZHm0J_U3JLoDnM3bjKUY4OIgsCbdwwPZ4_qpcUv5YyvzSlvldZ9CHZ-JRoAcoeVX9yb18Wt6Oss46Oz9bLN7Z3Zx2ZBNzw0K3eQnVV0IHmQeRZfW2rKl0pGkl3kzRNISZa3NR1QfP-jyvk6i5SIx3kqB6zlEZDeWqkP6bG_mFoU6I44G1AvEv1TnpS_oKTT0mOGjj7hshRwa4AORp8AnUalUtGnb2VhI-KaDU0x2vNqeezbOBllTKF4ZLftPaAEAWZZ9T-fbJLjjXJc_YmfDpItAV14x9dtGyhHk77BLkHUoCw/file\n",
            "Reusing existing connection to ucb75219c897d6d1b46c734915b3.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 805634 (787K) [application/octet-stream]\n",
            "Saving to: ‘model/wiki-ja.model.1’\n",
            "\n",
            "wiki-ja.model.1     100%[===================>] 786.75K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-05-04 23:06:51 (14.3 MB/s) - ‘model/wiki-ja.model.1’ saved [805634/805634]\n",
            "\n",
            "--2023-05-04 23:06:51--  https://www.dropbox.com/s/b464zufwp05pwz4/wiki-ja.vocab\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/b464zufwp05pwz4/wiki-ja.vocab [following]\n",
            "--2023-05-04 23:06:51--  https://www.dropbox.com/s/raw/b464zufwp05pwz4/wiki-ja.vocab\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucde1a7d86d92461b4fb547cf9e5.dl.dropboxusercontent.com/cd/0/inline/B7e1iyKPCSaidcBPrqf02u0ZtbsRPw7w6sVKuMqyDPu93IT6TIZaeboSF081wFA8llAcWhqh5snBiOT_b5IK-dvvZgOiFHXlQuQnFjJ7PmQZ9NDovFXETi5E7rUFkVRFf9TBgv9Q_KB6wMsIZIkfyAKzKEgoLhOejjCsw8DlI8vniQ/file# [following]\n",
            "--2023-05-04 23:06:52--  https://ucde1a7d86d92461b4fb547cf9e5.dl.dropboxusercontent.com/cd/0/inline/B7e1iyKPCSaidcBPrqf02u0ZtbsRPw7w6sVKuMqyDPu93IT6TIZaeboSF081wFA8llAcWhqh5snBiOT_b5IK-dvvZgOiFHXlQuQnFjJ7PmQZ9NDovFXETi5E7rUFkVRFf9TBgv9Q_KB6wMsIZIkfyAKzKEgoLhOejjCsw8DlI8vniQ/file\n",
            "Resolving ucde1a7d86d92461b4fb547cf9e5.dl.dropboxusercontent.com (ucde1a7d86d92461b4fb547cf9e5.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to ucde1a7d86d92461b4fb547cf9e5.dl.dropboxusercontent.com (ucde1a7d86d92461b4fb547cf9e5.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 595681 (582K) [text/plain]\n",
            "Saving to: ‘model/wiki-ja.vocab.1’\n",
            "\n",
            "wiki-ja.vocab.1     100%[===================>] 581.72K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-05-04 23:06:52 (10.7 MB/s) - ‘model/wiki-ja.vocab.1’ saved [595681/595681]\n",
            "\n",
            "--2023-05-04 23:06:52--  https://www.dropbox.com/s/bj6itc7edybfq6x/model.ckpt-1000000.data-00000-of-00001\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/bj6itc7edybfq6x/model.ckpt-1000000.data-00000-of-00001 [following]\n",
            "--2023-05-04 23:06:52--  https://www.dropbox.com/s/raw/bj6itc7edybfq6x/model.ckpt-1000000.data-00000-of-00001\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uce8afe29a55f23b4d7165120242.dl.dropboxusercontent.com/cd/0/inline/B7f5fmZF6mxSM25BdzNo6V-m2sVB6DNsyGepfxlRF3LVB6aLdTkBP83fVcJl6HuhMP1GgqJ8X7H6IIqfbDZ-xK0gf69pMeNAmPhBoX4O5rw34CAvT054-w1462cmVWNoCvJPR6mJn-c7vPofIhY_6QMs7Ef-gjbDB9YwYmZ08U8V9w/file# [following]\n",
            "--2023-05-04 23:06:53--  https://uce8afe29a55f23b4d7165120242.dl.dropboxusercontent.com/cd/0/inline/B7f5fmZF6mxSM25BdzNo6V-m2sVB6DNsyGepfxlRF3LVB6aLdTkBP83fVcJl6HuhMP1GgqJ8X7H6IIqfbDZ-xK0gf69pMeNAmPhBoX4O5rw34CAvT054-w1462cmVWNoCvJPR6mJn-c7vPofIhY_6QMs7Ef-gjbDB9YwYmZ08U8V9w/file\n",
            "Resolving uce8afe29a55f23b4d7165120242.dl.dropboxusercontent.com (uce8afe29a55f23b4d7165120242.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uce8afe29a55f23b4d7165120242.dl.dropboxusercontent.com (uce8afe29a55f23b4d7165120242.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/B7ef2XXeAF-LTrqPNnIvRDzwavdnW2dUNjhnO-VXhFKDYWK7SYGBSDqpg_NyW0fMnIIo5Aq3xyyWkBVptmhjjgVGxonaJHAnjswxr-wxlSPF4wTBzGrlZ1lcXu4r_0eybsZyWFFReeKBrGXi9E59aD1gFbHEt08VNfHfiZSUPYOHjfCq0t2rlS1f52Jt8WkAIvjfPfmVo28gdBh575ik7j96HU1cLyydToQwiHi3Y-m8AsEBbjCKfonFy7zCCI__njxgrz2q61Ke0TwpaqfQW46AKkaLMcMd1NyWnGhpBWyV0ZZ88zcWtaDrbZSuFkVYKEVXlJqIQifJRcdgdVfqdnbeFMNhQS4H2kDtldGBhbD1Q3eyVqzWeHFNkI12cLTqP7E2YlROn1e8BZYEr3JRBToZXXuUJ5Gfhchsd4iJUa3KXg/file [following]\n",
            "--2023-05-04 23:06:53--  https://uce8afe29a55f23b4d7165120242.dl.dropboxusercontent.com/cd/0/inline2/B7ef2XXeAF-LTrqPNnIvRDzwavdnW2dUNjhnO-VXhFKDYWK7SYGBSDqpg_NyW0fMnIIo5Aq3xyyWkBVptmhjjgVGxonaJHAnjswxr-wxlSPF4wTBzGrlZ1lcXu4r_0eybsZyWFFReeKBrGXi9E59aD1gFbHEt08VNfHfiZSUPYOHjfCq0t2rlS1f52Jt8WkAIvjfPfmVo28gdBh575ik7j96HU1cLyydToQwiHi3Y-m8AsEBbjCKfonFy7zCCI__njxgrz2q61Ke0TwpaqfQW46AKkaLMcMd1NyWnGhpBWyV0ZZ88zcWtaDrbZSuFkVYKEVXlJqIQifJRcdgdVfqdnbeFMNhQS4H2kDtldGBhbD1Q3eyVqzWeHFNkI12cLTqP7E2YlROn1e8BZYEr3JRBToZXXuUJ5Gfhchsd4iJUa3KXg/file\n",
            "Reusing existing connection to uce8afe29a55f23b4d7165120242.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1334916128 (1.2G) [application/octet-stream]\n",
            "Saving to: ‘model/model.ckpt-1000000.data-00000-of-00001.1’\n",
            "\n",
            "model.ckpt-1000000. 100%[===================>]   1.24G   112MB/s    in 12s     \n",
            "\n",
            "2023-05-04 23:07:06 (104 MB/s) - ‘model/model.ckpt-1000000.data-00000-of-00001.1’ saved [1334916128/1334916128]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "nzd02um0TS_p",
        "outputId": "a87bae50-06d5-4d49-bda4-e4126c25a205",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "! pip install sentencepiece\n",
        "! pip install configparser"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.10/dist-packages (5.3.0)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "JRw8a9yQe1w5"
      },
      "cell_type": "markdown",
      "source": [
        "#### Modeling"
      ]
    },
    {
      "metadata": {
        "id": "NhwkUvRKe1w7"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import configparser\n",
        "import collections\n",
        "import copy\n",
        "import csv\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import math\n",
        "import re\n",
        "import numpy as np\n",
        "import sentencepiece as sp\n",
        "import six\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class BertConfig(object):\n",
        "\n",
        "  def __init__(self, vocab_size, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, hidden_act=\"gelu\", \n",
        "               hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=16, initializer_range=0.02):\n",
        "    \"\"\"\n",
        "      vocab_size: `inputs_ids`の語彙数\n",
        "      hidden_size: encoder層とpooler層の隠れ層次元数, 論文値 768\n",
        "      num_hidden_layers: Transformer encoder層の数, 論文値 12\n",
        "      num_attention_heads: Transformer encoder内部のAttantion headの数, 論文値 12\n",
        "      intermediate_size: Transformer encoder内部のintermediate層(例 feed-forward)の次元数\n",
        "      hidden_act: encoderとpoolerでの活性化関数, 論文値 gelu\n",
        "      hidden_dropout_prob: embeddings, encoder, poolerでのdropout rate\n",
        "      attention_probs_dropout_prob: attention層でのdropout rate\n",
        "      max_position_embeddings: 入力系列の最大長, 論文値 512\n",
        "      type_vocab_size: \n",
        "      initializer_range: 重みパラメタを初期化する際の分散\n",
        "    \"\"\"\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.num_attention_heads = num_attention_heads\n",
        "    self.hidden_act = hidden_act\n",
        "    self.intermediate_size = intermediate_size\n",
        "    self.hidden_dropout_prob = hidden_dropout_prob\n",
        "    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "    self.max_position_embeddings = max_position_embeddings\n",
        "    self.type_vocab_size = type_vocab_size\n",
        "    self.initializer_range = initializer_range\n",
        "\n",
        "  @classmethod\n",
        "  def from_dict(cls, json_object):\n",
        "    \"\"\"パラメタ情報を持つdictionaryからBERT configを生成する\"\"\"\n",
        "    config = BertConfig(vocab_size=None)\n",
        "    for (key, value) in six.iteritems(json_object):\n",
        "      config.__dict__[key] = value\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def from_json_file(cls, json_file):\n",
        "    \"\"\"パラメタ情報を持つjsonからBERT configを生成する\"\"\"\n",
        "    with tf.gfile.GFile(json_file, \"r\") as reader:\n",
        "      text = reader.read()\n",
        "    return cls.from_dict(json.loads(text))\n",
        "\n",
        "  def to_dict(self):\n",
        "    output = copy.deepcopy(self.__dict__)\n",
        "    return output\n",
        "\n",
        "  def to_json_string(self):\n",
        "    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class BertModel(object):\n",
        "  \"\"\"BERT model (\"Bidirectional Encoder Representations from Transformers\").\n",
        "\n",
        "  Example usage:\n",
        "\n",
        "  ```python\n",
        "  # Already been converted into WordPiece token ids\n",
        "  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
        "  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n",
        "  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
        "    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "  model = modeling.BertModel(config=config, is_training=True,\n",
        "    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "  label_embeddings = tf.get_variable(...)\n",
        "  pooled_output = model.get_pooled_output()\n",
        "  logits = tf.matmul(pooled_output, label_embeddings)\n",
        "  ...\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config, is_training, input_ids, input_mask=None, token_type_ids=None, use_one_hot_embeddings=False, scope=None):\n",
        "    \"\"\"Constructor for BertModel.\n",
        "      config: `BertConfig` インスタンス\n",
        "      is_training: bool. trainingの際はtrue, evalの際はfalse. dropoutの有無を制御する\n",
        "      input_ids: int32 Tensor of shape [batch_size, seq_length]\n",
        "      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length]\n",
        "      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
        "      use_one_hot_embeddings: (optional) bool. true:one-hot word embeddings, false:tf.embedding_lookup()\n",
        "      scope: (optional) variable scope. Defaults to \"bert\".\n",
        "    \"\"\"\n",
        "    config = copy.deepcopy(config)\n",
        "    if not is_training:\n",
        "      config.hidden_dropout_prob = 0.0\n",
        "      config.attention_probs_dropout_prob = 0.0\n",
        "\n",
        "    input_shape = get_shape_list(input_ids, expected_rank=2)\n",
        "    batch_size = input_shape[0]\n",
        "    seq_length = input_shape[1]\n",
        "\n",
        "    if input_mask is None:\n",
        "      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n",
        "\n",
        "    if token_type_ids is None:\n",
        "      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n",
        "\n",
        "    with tf.variable_scope(scope, default_name=\"bert\"):\n",
        "      with tf.variable_scope(\"embeddings\"):\n",
        "        # word idsにembedding lookupを適用\n",
        "        (self.embedding_output, self.embedding_table) = embedding_lookup(\n",
        "            input_ids=input_ids, vocab_size=config.vocab_size, embedding_size=config.hidden_size,\n",
        "            initializer_range=config.initializer_range, word_embedding_name=\"word_embeddings\", use_one_hot_embeddings=use_one_hot_embeddings\n",
        "        )\n",
        "        # positional embedding => token type embedding => layer normalize & dropout\n",
        "        self.embedding_output = embedding_postprocessor(\n",
        "            input_tensor=self.embedding_output, use_token_type=True, token_type_ids=token_type_ids, token_type_vocab_size=config.type_vocab_size,token_type_embedding_name=\"token_type_embeddings\",\n",
        "            use_position_embeddings=True, position_embedding_name=\"position_embeddings\", initializer_range=config.initializer_range,\n",
        "            max_position_embeddings=config.max_position_embeddings, dropout_prob=config.hidden_dropout_prob\n",
        "        )\n",
        "\n",
        "      with tf.variable_scope(\"encoder\"):\n",
        "        # [batch_size, seq_length] => [batch_size, seq_length, seq_length]に変換する. これはAttention行列の生成に使用する\n",
        "        attention_mask = create_attention_mask_from_input_mask(input_ids, input_mask)\n",
        "\n",
        "        # Transformerを動かす\n",
        "        self.all_encoder_layers = transformer_model(\n",
        "            input_tensor=self.embedding_output,\n",
        "            attention_mask=attention_mask,\n",
        "            hidden_size=config.hidden_size,\n",
        "            num_hidden_layers=config.num_hidden_layers,\n",
        "            num_attention_heads=config.num_attention_heads,\n",
        "            intermediate_size=config.intermediate_size,\n",
        "            intermediate_act_fn=get_activation(config.hidden_act),\n",
        "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
        "            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
        "            initializer_range=config.initializer_range,\n",
        "            do_return_all_layers=True)\n",
        "\n",
        "      self.sequence_output = self.all_encoder_layers[-1] # [batch_size, seq_length, hidden_size]\n",
        "    \n",
        "      # `pooler`はencodeされた系列[batch_size, seq_length, hidden_size] を [batch_size, hidden_size]に変換する.\n",
        "      # segmentレベルの分類タスクを解く際に必要になる\n",
        "      with tf.variable_scope(\"pooler\"):\n",
        "        # 最初のトークン[CLS]に当たる内部状態ベクトルを取得する\n",
        "        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1) # 最初のトークンを取得して次元削減\n",
        "        self.pooled_output = tf.layers.dense(first_token_tensor, config.hidden_size, activation=tf.tanh, kernel_initializer=create_initializer(config.initializer_range))\n",
        "\n",
        "  def get_pooled_output(self):\n",
        "    return self.pooled_output\n",
        "\n",
        "  def get_sequence_output(self):\n",
        "    \"\"\"\n",
        "    最後のencoder層の出力を得る\n",
        "\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, seq_length, hidden_size]\n",
        "    \"\"\"\n",
        "    return self.sequence_output\n",
        "\n",
        "  def get_all_encoder_layers(self):\n",
        "    return self.all_encoder_layers\n",
        "\n",
        "  def get_embedding_output(self):\n",
        "    \"\"\"\n",
        "    embedding lookupの出力を得る (transformerへの入力になる)\n",
        "    word embedding + positional embedding + token type embeddingを計算したのちlayer normalizationをかける\n",
        "    これはtransformerの入力になる\n",
        "\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, seq_length, hidden_size]\n",
        "    \"\"\"\n",
        "    return self.embedding_output\n",
        "\n",
        "  def get_embedding_table(self):\n",
        "    return self.embedding_table\n",
        "\n",
        "\n",
        "# 活性化関数\n",
        "def gelu(x):\n",
        "  \"\"\"\n",
        "  滑らかなReLu\n",
        "  \"\"\"\n",
        "  cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
        "  return x * cdf\n",
        "\n",
        "\n",
        "def get_activation(activation_string):\n",
        "  \"\"\"\n",
        "  活性化関数の名前をtfメソッドに変換する\n",
        "  e.g., \"relu\" => `tf.nn.relu`.\n",
        "\n",
        "  Returns:\n",
        "    If `activation_string` is None, empty, or \"linear\", this will return None.\n",
        "    If `activation_string` is not a string, it will return `activation_string`.\n",
        "  \"\"\"\n",
        "  if not isinstance(activation_string, six.string_types):\n",
        "    return activation_string\n",
        "\n",
        "  if not activation_string:\n",
        "    return None\n",
        "\n",
        "  act = activation_string.lower()\n",
        "  if act == \"linear\":\n",
        "    return None\n",
        "  elif act == \"relu\":\n",
        "    return tf.nn.relu\n",
        "  elif act == \"gelu\":\n",
        "    return gelu\n",
        "  elif act == \"tanh\":\n",
        "    return tf.tanh\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported activation: %s\" % act)\n",
        "\n",
        "\n",
        "def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n",
        "  \"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"\n",
        "  assignment_map = {}\n",
        "  initialized_variable_names = {}\n",
        "\n",
        "  name_to_variable = collections.OrderedDict()\n",
        "  for var in tvars:\n",
        "    name = var.name\n",
        "    m = re.match(\"^(.*):\\\\d+$\", name)\n",
        "    if m is not None:\n",
        "      name = m.group(1)\n",
        "    name_to_variable[name] = var\n",
        "\n",
        "  init_vars = tf.train.list_variables(init_checkpoint)\n",
        "\n",
        "  assignment_map = collections.OrderedDict()\n",
        "  for x in init_vars:\n",
        "    (name, var) = (x[0], x[1])\n",
        "    if name not in name_to_variable:\n",
        "      continue\n",
        "    assignment_map[name] = name\n",
        "    initialized_variable_names[name] = 1\n",
        "    initialized_variable_names[name + \":0\"] = 1\n",
        "\n",
        "  return (assignment_map, initialized_variable_names)\n",
        "\n",
        "\n",
        "def dropout(input_tensor, dropout_prob):\n",
        "  if dropout_prob is None or dropout_prob == 0.0:\n",
        "    return input_tensor\n",
        "\n",
        "  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n",
        "  return output\n",
        "\n",
        "\n",
        "def layer_norm(input_tensor, name=None):\n",
        "  \"\"\"テンソルの最終次元に対してlayer normalizationを行う\"\"\"\n",
        "  return tf.contrib.layers.layer_norm(inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n",
        "\n",
        "\n",
        "def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n",
        "  output_tensor = layer_norm(input_tensor, name)\n",
        "  output_tensor = dropout(output_tensor, dropout_prob)\n",
        "  return output_tensor\n",
        "\n",
        "\n",
        "def create_initializer(initializer_range=0.02):\n",
        "  \"\"\"与えられたレンジでの切断正規分布を生成する\"\"\"\n",
        "  return tf.truncated_normal_initializer(stddev=initializer_range)\n",
        "\n",
        "\n",
        "def embedding_lookup(input_ids, vocab_size, embedding_size=128, initializer_range=0.02,\n",
        "                     word_embedding_name=\"word_embeddings\", use_one_hot_embeddings=False):\n",
        "  \"\"\"\n",
        "  該当するword idのembeddingを検索する\n",
        "  \n",
        "    input_ids: int32 Tensor of shape [batch_size, seq_length]. word ids.\n",
        "    vocab_size: int.\n",
        "    embedding_size: int.\n",
        "    initializer_range: float. embedding重みパラメタ初期化の際の分散\n",
        "    word_embedding_name: string. embedding tableの名前\n",
        "    use_one_hot_embeddings: bool. True: use one-hot method for word embeddings. False: use `tf.gather()`.\n",
        "\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, seq_length, embedding_size].\n",
        "  \"\"\"\n",
        "  if input_ids.shape.ndims == 2: # 入力には３次元のもの[batch_size, seq_length, num_inputs]を想定している\n",
        "    input_ids = tf.expand_dims(input_ids, axis=[-1]) # ２次元のものが来た場合には[batch_size, seq_length, 1]に変換する\n",
        "\n",
        "  embedding_table = tf.get_variable(name=word_embedding_name, shape=[vocab_size, embedding_size], initializer=create_initializer(initializer_range))\n",
        "\n",
        "  flat_input_ids = tf.reshape(input_ids, [-1])\n",
        "  if use_one_hot_embeddings:\n",
        "    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n",
        "    output = tf.matmul(one_hot_input_ids, embedding_table)\n",
        "  else:\n",
        "    output = tf.gather(embedding_table, flat_input_ids)\n",
        "\n",
        "  input_shape = get_shape_list(input_ids)\n",
        "\n",
        "  output = tf.reshape(output, input_shape[0:-1] + [input_shape[-1] * embedding_size])\n",
        "  return (output, embedding_table)\n",
        "\n",
        "\n",
        "def embedding_postprocessor(input_tensor, use_token_type=False, token_type_ids=None, token_type_vocab_size=16, token_type_embedding_name=\"token_type_embeddings\",\n",
        "                            use_position_embeddings=True, position_embedding_name=\"position_embeddings\", initializer_range=0.02, max_position_embeddings=512, dropout_prob=0.1):\n",
        "  \"\"\"\n",
        "  word embedding tensorにいろいろと後処理を行うメソッド\n",
        "\n",
        "    input_tensor: float Tensor of shape [batch_size, seq_length, embedding_size].\n",
        "    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n",
        "    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length]. `use_token_type`=Trueが必要\n",
        "    token_type_vocab_size: int.\n",
        "    token_type_embedding_name: string. token type embedding table名前\n",
        "    use_position_embeddings: bool. Whether to add position embeddings for the position of each token in the sequence.\n",
        "    position_embedding_name: string. positional embedding tableの名前\n",
        "    initializer_range: float. 重みパラメタ初期化の際の分散\n",
        "    max_position_embeddings: int. 入力系列の最大長\n",
        "    dropout_prob: float. 最終出力のtensorにかけられるdropout rate\n",
        "\n",
        "  Returns:\n",
        "    `input_tensor`と同じサイズのtensorを返す\n",
        "  \"\"\"\n",
        "  input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
        "  batch_size = input_shape[0]\n",
        "  seq_length = input_shape[1]\n",
        "  width = input_shape[2]\n",
        "\n",
        "  output = input_tensor\n",
        "\n",
        "  if use_token_type: # token_type_idsをembeddingsに足し合わせるか\n",
        "    if token_type_ids is None:\n",
        "      raise ValueError(\"`token_type_ids` must be specified if `use_token_type` is True.\")\n",
        "    token_type_table = tf.get_variable(name=token_type_embedding_name, shape=[token_type_vocab_size, width], initializer=create_initializer(initializer_range))\n",
        "    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n",
        "    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size) # onehotのほうが計算が早い\n",
        "    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n",
        "    token_type_embeddings = tf.reshape(token_type_embeddings, [batch_size, seq_length, width])\n",
        "    output += token_type_embeddings\n",
        "\n",
        "  if use_position_embeddings: # position embeddingsをembbeddingsに足し合わせるか\n",
        "    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n",
        "    with tf.control_dependencies([assert_op]):\n",
        "      full_position_embeddings = tf.get_variable(name=position_embedding_name, shape=[max_position_embeddings, width], initializer=create_initializer(initializer_range))\n",
        "      position_embeddings = tf.slice(full_position_embeddings, [0, 0], [seq_length, -1])\n",
        "      num_dims = len(output.shape.as_list())\n",
        "\n",
        "      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n",
        "      # we broadcast among the first dimensions, which is typically just\n",
        "      # the batch size.\n",
        "      position_broadcast_shape = []\n",
        "      for _ in range(num_dims - 2):\n",
        "        position_broadcast_shape.append(1)\n",
        "      position_broadcast_shape.extend([seq_length, width])\n",
        "      position_embeddings = tf.reshape(position_embeddings, position_broadcast_shape)\n",
        "      output += position_embeddings\n",
        "\n",
        "  output = layer_norm_and_dropout(output, dropout_prob)\n",
        "  return output\n",
        "\n",
        "\n",
        "\n",
        "def create_attention_mask_from_input_mask(from_tensor, to_mask):\n",
        "  \"\"\"\n",
        "  2Dのマスク入力から3Dのマスクtensorを出力する\n",
        "\n",
        "  Args:\n",
        "    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n",
        "    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n",
        "\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n",
        "  \"\"\"\n",
        "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
        "  batch_size = from_shape[0]\n",
        "  from_seq_length = from_shape[1]\n",
        "\n",
        "  to_shape = get_shape_list(to_mask, expected_rank=2)\n",
        "  to_seq_length = to_shape[1]\n",
        "\n",
        "  to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n",
        "  broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n",
        "  mask = broadcast_ones * to_mask\n",
        "\n",
        "  return mask\n",
        "\n",
        "\n",
        "def attention_layer(from_tensor, to_tensor, attention_mask=None, num_attention_heads=1, size_per_head=512, query_act=None, key_act=None, value_act=None,\n",
        "                    attention_probs_dropout_prob=0.0, initializer_range=0.02, do_return_2d_tensor=False, batch_size=None, from_seq_length=None, to_seq_length=None):\n",
        "  \"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n",
        "  `from_tensor` と `to_tensor`が同じものの場合self-attentionになる。\n",
        "  `from_tensor` => query\n",
        "  `to_tensor` => key, value\n",
        "  それぞれのtensor shapeは[batch_size, seq_length, size_per_head]\n",
        "\n",
        "    from_tensor: float Tensor of shape [batch_size, from_seq_length, from_width].\n",
        "    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n",
        "    attention_mask: (optional) int32 Tensor of shape [batch_size, from_seq_length, to_seq_length]. 値は0 or 1. 0はmaskする要素に当たる\n",
        "    num_attention_heads: int. attention head の個数. default 1\n",
        "    size_per_head: int. attention headのhidden size\n",
        "    query_act: (optional) Activation function for the query transform.\n",
        "    key_act: (optional) Activation function for the key transform.\n",
        "    value_act: (optional) Activation function for the value transform.\n",
        "    attention_probs_dropout_prob: (optional) float. Dropout probability of the attention probabilities.\n",
        "    initializer_range: float. Range of the weight initializer.\n",
        "    do_return_2d_tensor: bool. True: output shape = [batch_size * from_seq_length, num_attention_heads * size_per_head]\n",
        "                              False: output shape = [batch_size, from_seq_length, num_attention_heads * size_per_head]\n",
        "    batch_size: (Optional) int. 入力が2Dの場合は3Dの`from_tensor`と`to_tensor`のbatch_sizeになる\n",
        "    from_seq_length: (Optional) 入力が2Dの場合は3Dの`from_tensor`のseq_lengthになる\n",
        "    to_seq_length: (Optional) 入力が2Dの場合は3Dの`to_tensor`のseq_lengthになる\n",
        "\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, from_seq_length, num_attention_heads * size_per_head]\n",
        "    (`do_return_2d_tensor`=Trueの場合は[batch_size * from_seq_length, num_attention_heads * size_per_head]).\n",
        "  \"\"\"\n",
        "\n",
        "  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,seq_length, width):\n",
        "    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n",
        "    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
        "    return output_tensor\n",
        "\n",
        "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
        "  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n",
        "\n",
        "  if len(from_shape) != len(to_shape):\n",
        "    raise ValueError(\"The rank of `from_tensor` must match the rank of `to_tensor`.\")\n",
        "\n",
        "  if len(from_shape) == 3:\n",
        "    batch_size = from_shape[0]\n",
        "    from_seq_length = from_shape[1]\n",
        "    to_seq_length = to_shape[1]\n",
        "  elif len(from_shape) == 2:\n",
        "    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n",
        "      raise ValueError(\n",
        "          \"When passing in rank 2 tensors to attention_layer, the values \"\n",
        "          \"for `batch_size`, `from_seq_length`, and `to_seq_length` \"\n",
        "          \"must all be specified.\")\n",
        "\n",
        "  #   B = batch size (number of sequences)\n",
        "  #   F = `from_tensor` sequence length\n",
        "  #   T = `to_tensor` sequence length\n",
        "  #   N = `num_attention_heads`\n",
        "  #   H = `size_per_head`\n",
        "  from_tensor_2d = reshape_to_matrix(from_tensor)\n",
        "  to_tensor_2d = reshape_to_matrix(to_tensor)\n",
        "\n",
        "  query_layer = tf.layers.dense(\n",
        "      from_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=query_act,\n",
        "      name=\"query\",\n",
        "      kernel_initializer=create_initializer(initializer_range)) # [B*F, N*H]\n",
        "\n",
        "  key_layer = tf.layers.dense(\n",
        "      to_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=key_act,\n",
        "      name=\"key\",\n",
        "      kernel_initializer=create_initializer(initializer_range)) # [B*T, N*H]\n",
        "\n",
        "  value_layer = tf.layers.dense(\n",
        "      to_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=value_act,\n",
        "      name=\"value\",\n",
        "      kernel_initializer=create_initializer(initializer_range)) # [B*T, N*H]\n",
        "\n",
        "  query_layer = transpose_for_scores(query_layer, batch_size,num_attention_heads, from_seq_length, size_per_head) # [B, N, F, H]\n",
        "\n",
        "  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads, to_seq_length, size_per_head) # [B, N, T, H]\n",
        "\n",
        "  # queryとkeyの内積をとってattention scoreを取得する\n",
        "  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True) # [B, N, F, T]\n",
        "  attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(size_per_head))) # scaling\n",
        "\n",
        "  if attention_mask is not None:\n",
        "    attention_mask = tf.expand_dims(attention_mask, axis=[1]) # [B, 1, F, T]\n",
        "\n",
        "    # attention_maskが0の箇所は負の巨大な数値に飛ばすことによってsoftmaxを事実上0にする\n",
        "    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n",
        "    attention_scores += adder\n",
        "\n",
        "  # attention scoreを確率に正規化する\n",
        "  attention_probs = tf.nn.softmax(attention_scores) # [B, N, F, T]\n",
        "  # attentionに対してdropoutを適用する\n",
        "  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n",
        "\n",
        "  value_layer = tf.reshape(value_layer, [batch_size, to_seq_length, num_attention_heads, size_per_head]) # [B, T, N, H]\n",
        "  value_layer = tf.transpose(value_layer, [0, 2, 1, 3]) # [B, N, T, H]\n",
        "\n",
        "  context_layer = tf.matmul(attention_probs, value_layer) # [B, N, F, H]\n",
        "  context_layer = tf.transpose(context_layer, [0, 2, 1, 3]) # [B, F, N, H]\n",
        "\n",
        "  if do_return_2d_tensor:\n",
        "    context_layer = tf.reshape(context_layer, [batch_size * from_seq_length, num_attention_heads * size_per_head]) # [B*F, N*H]\n",
        "  else:\n",
        "    context_layer = tf.reshape(context_layer, [batch_size, from_seq_length, num_attention_heads * size_per_head]) # [B, F, N*H]\n",
        "  return context_layer\n",
        "\n",
        "\n",
        "def transformer_model(input_tensor, attention_mask=None, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072,\n",
        "                      intermediate_act_fn=gelu, hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, initializer_range=0.02, do_return_all_layers=False):\n",
        "  \"\"\"Multi-headed, multi-layer Transformer from \"Attention is All You Need\".\n",
        "  オリジナルのtransformerの実装とほぼ同じ\n",
        "\n",
        "  Also see:\n",
        "  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n",
        "\n",
        "    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n",
        "    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length, seq_length]. 1はAttention対象、0は対象外\n",
        "    hidden_size: int. 隠れ層次元数\n",
        "    num_hidden_layers: int. Transformerの隠れ層の数\n",
        "    num_attention_heads: int. Transformer内部のAttention headの数\n",
        "    intermediate_size: int. feed forward layerの隠れ層次元数\n",
        "    intermediate_act_fn: function. feed forward layerの活性化関数\n",
        "    hidden_dropout_prob: float. hidden layersのdropout rate\n",
        "    attention_probs_dropout_prob: float. attentionのdropout rate\n",
        "    initializer_range: float. 初期化に用いる切断正規分布のレンジ\n",
        "    do_return_all_layer: true->return all layers, false->return final layer\n",
        "\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, seq_length, hidden_size], the final hidden layer of the Transformer.\n",
        "  \"\"\"\n",
        "  if hidden_size % num_attention_heads != 0:\n",
        "    raise ValueError(\n",
        "        \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "        \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
        "\n",
        "  attention_head_size = int(hidden_size / num_attention_heads)\n",
        "  input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
        "  batch_size = input_shape[0]\n",
        "  seq_length = input_shape[1]\n",
        "  input_width = input_shape[2]\n",
        "\n",
        "  # Transformerはresidual addを全layerに対して行うので、入力は隠れ層次元と同じ必要がある.\n",
        "  if input_width != hidden_size:\n",
        "    raise ValueError(\"The width of the input tensor (%d) != hidden size (%d)\" % (input_width, hidden_size))\n",
        "\n",
        "  # 2D形式のtensorを保持する. TPUではreshapeが自在に行えないことが背景にある.\n",
        "  prev_output = reshape_to_matrix(input_tensor)\n",
        "\n",
        "  all_layer_outputs = []\n",
        "  for layer_idx in range(num_hidden_layers):\n",
        "    with tf.variable_scope(\"layer_%d\" % layer_idx):\n",
        "      layer_input = prev_output\n",
        "\n",
        "      # attention layerの定義\n",
        "      with tf.variable_scope(\"attention\"):\n",
        "        attention_heads = []\n",
        "        with tf.variable_scope(\"self\"):\n",
        "          attention_head = attention_layer(from_tensor=layer_input, to_tensor=layer_input, attention_mask=attention_mask, num_attention_heads=num_attention_heads,\n",
        "              size_per_head=attention_head_size, attention_probs_dropout_prob=attention_probs_dropout_prob, initializer_range=initializer_range, do_return_2d_tensor=True,\n",
        "              batch_size=batch_size, from_seq_length=seq_length, to_seq_length=seq_length)\n",
        "          attention_heads.append(attention_head)\n",
        "\n",
        "        # attention headsの出力を結合\n",
        "        attention_output = None\n",
        "        if len(attention_heads) == 1:\n",
        "          attention_output = attention_heads[0]\n",
        "        else:\n",
        "          attention_output = tf.concat(attention_heads, axis=-1)\n",
        "\n",
        "        # FFN->dropout->residual add->layer normalize\n",
        "        with tf.variable_scope(\"output\"):\n",
        "          attention_output = tf.layers.dense(attention_output, hidden_size, kernel_initializer=create_initializer(initializer_range))\n",
        "          attention_output = dropout(attention_output, hidden_dropout_prob)\n",
        "          attention_output = layer_norm(attention_output + layer_input)\n",
        "\n",
        "      # 活性化関数は中間FFNのみにかける\n",
        "      with tf.variable_scope(\"intermediate\"):\n",
        "        intermediate_output = tf.layers.dense(\n",
        "            attention_output,\n",
        "            intermediate_size,\n",
        "            activation=intermediate_act_fn,\n",
        "            kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "      # 中間FFNの出力をattention layerに流し、dropout->residual->layer norm\n",
        "      with tf.variable_scope(\"output\"):\n",
        "        layer_output = tf.layers.dense(\n",
        "            intermediate_output,\n",
        "            hidden_size,\n",
        "            kernel_initializer=create_initializer(initializer_range))\n",
        "        layer_output = dropout(layer_output, hidden_dropout_prob)\n",
        "        layer_output = layer_norm(layer_output + attention_output)\n",
        "        prev_output = layer_output\n",
        "        all_layer_outputs.append(layer_output)\n",
        "\n",
        "  if do_return_all_layers:\n",
        "    final_outputs = []\n",
        "    for layer_output in all_layer_outputs:\n",
        "      final_output = reshape_from_matrix(layer_output, input_shape)\n",
        "      final_outputs.append(final_output)\n",
        "    return final_outputs\n",
        "  else:\n",
        "    final_output = reshape_from_matrix(prev_output, input_shape)\n",
        "    return final_output\n",
        "\n",
        "\n",
        "def get_shape_list(tensor, expected_rank=None, name=None):\n",
        "  \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
        "\n",
        "    tensor: A tf.Tensor object. shapeを取得する対象\n",
        "    expected_rank: (optional) int. shapeを取得した結果こちらのexpected_shapeと異なっていた場合は例外処理を行う\n",
        "    name: (optional) error messageのためのtensorの名前\n",
        "\n",
        "  Returns:\n",
        "    A list of dimensions of the shape of tensor. All static dimensions will\n",
        "    be returned as python integers, and dynamic dimensions will be returned\n",
        "    as tf.Tensor scalars.\n",
        "  \"\"\"\n",
        "  if name is None:\n",
        "    name = tensor.name\n",
        "\n",
        "  if expected_rank is not None:\n",
        "    assert_rank(tensor, expected_rank, name)\n",
        "\n",
        "  shape = tensor.shape.as_list()\n",
        "\n",
        "  non_static_indexes = []\n",
        "  for (index, dim) in enumerate(shape):\n",
        "    if dim is None:\n",
        "      non_static_indexes.append(index)\n",
        "\n",
        "  if not non_static_indexes:\n",
        "    return shape\n",
        "\n",
        "  dyn_shape = tf.shape(tensor)\n",
        "  for index in non_static_indexes:\n",
        "    shape[index] = dyn_shape[index]\n",
        "  return shape\n",
        "\n",
        "\n",
        "def reshape_to_matrix(input_tensor):\n",
        "  \"\"\"rankが2より大きいはあいはrank2のtensorに変形する\"\"\"\n",
        "  ndims = input_tensor.shape.ndims\n",
        "  if ndims < 2:\n",
        "    raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n",
        "                     (input_tensor.shape))\n",
        "  if ndims == 2:\n",
        "    return input_tensor\n",
        "\n",
        "  width = input_tensor.shape[-1]\n",
        "  output_tensor = tf.reshape(input_tensor, [-1, width])\n",
        "  return output_tensor\n",
        "\n",
        "\n",
        "def reshape_from_matrix(output_tensor, orig_shape_list):\n",
        "  \"\"\"rank2のtensorをoriginal_shapeに戻す\"\"\"\n",
        "  if len(orig_shape_list) == 2:\n",
        "    return output_tensor\n",
        "\n",
        "  output_shape = get_shape_list(output_tensor)\n",
        "\n",
        "  orig_dims = orig_shape_list[0:-1]\n",
        "  width = output_shape[-1]\n",
        "\n",
        "  return tf.reshape(output_tensor, orig_dims + [width])\n",
        "\n",
        "\n",
        "def assert_rank(tensor, expected_rank, name=None):\n",
        "  \"\"\"Raises an exception if the tensor rank is not of the expected rank.\n",
        "\n",
        "    tensor: rankをチェックするtensor入力\n",
        "    expected_rank: Python integer or list of integers, expected rank.\n",
        "    name: (optional) error messageのためのtensorの名前\n",
        "\n",
        "  Raises:\n",
        "    ValueError: expected_shapeが実際のtensorのshapeと合わない場合はエラー\n",
        "  \"\"\"\n",
        "  if name is None:\n",
        "    name = tensor.name\n",
        "\n",
        "  expected_rank_dict = {}\n",
        "  if isinstance(expected_rank, six.integer_types):\n",
        "    expected_rank_dict[expected_rank] = True\n",
        "  else:\n",
        "    for x in expected_rank:\n",
        "      expected_rank_dict[x] = True\n",
        "\n",
        "  actual_rank = tensor.shape.ndims\n",
        "  if actual_rank not in expected_rank_dict:\n",
        "    scope_name = tf.get_variable_scope().name\n",
        "    raise ValueError(\n",
        "        \"For the tensor `%s` in scope `%s`, the actual rank \"\n",
        "        \"`%d` (shape = %s) is not equal to the expected rank `%s`\" %\n",
        "        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nKkprXeze1w_"
      },
      "cell_type": "markdown",
      "source": [
        "#### Optimizer"
      ]
    },
    {
      "metadata": {
        "id": "KUq2egKce1xA"
      },
      "cell_type": "code",
      "source": [
        "def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n",
        "  \"\"\"Creates an optimizer training op.\"\"\"\n",
        "  global_step = tf.train.get_or_create_global_step()\n",
        "\n",
        "  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
        "\n",
        "  # Implements linear decay of the learning rate.\n",
        "  learning_rate = tf.train.polynomial_decay(\n",
        "      learning_rate,\n",
        "      global_step,\n",
        "      num_train_steps,\n",
        "      end_learning_rate=0.0,\n",
        "      power=1.0,\n",
        "      cycle=False)\n",
        "\n",
        "  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n",
        "  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
        "  if num_warmup_steps:\n",
        "    global_steps_int = tf.cast(global_step, tf.int32)\n",
        "    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
        "\n",
        "    global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
        "    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
        "\n",
        "    warmup_percent_done = global_steps_float / warmup_steps_float\n",
        "    warmup_learning_rate = init_lr * warmup_percent_done\n",
        "\n",
        "    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
        "    learning_rate = (\n",
        "        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
        "\n",
        "  # It is recommended that you use this optimizer for fine tuning, since this\n",
        "  # is how the model was trained (note that the Adam m/v variables are NOT\n",
        "  # loaded from init_checkpoint.)\n",
        "  optimizer = AdamWeightDecayOptimizer(\n",
        "      learning_rate=learning_rate,\n",
        "      weight_decay_rate=0.01,\n",
        "      beta_1=0.9,\n",
        "      beta_2=0.999,\n",
        "      epsilon=1e-6,\n",
        "      exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
        "\n",
        "  if use_tpu:\n",
        "    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "  tvars = tf.trainable_variables()\n",
        "  grads = tf.gradients(loss, tvars)\n",
        "\n",
        "  # This is how the model was pre-trained.\n",
        "  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
        "\n",
        "  train_op = optimizer.apply_gradients(\n",
        "      zip(grads, tvars), global_step=global_step)\n",
        "\n",
        "  # Normally the global step update is done inside of `apply_gradients`.\n",
        "  # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use\n",
        "  # a different optimizer, you should probably take this line out.\n",
        "  new_global_step = global_step + 1\n",
        "  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
        "  return train_op\n",
        "\n",
        "\n",
        "class AdamWeightDecayOptimizer(tf.keras.optimizers.Optimizer):\n",
        "  \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               learning_rate,\n",
        "               weight_decay_rate=0.0,\n",
        "               beta_1=0.9,\n",
        "               beta_2=0.999,\n",
        "               epsilon=1e-6,\n",
        "               exclude_from_weight_decay=None,\n",
        "               name=\"AdamWeightDecayOptimizer\"):\n",
        "    \"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"\n",
        "    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weight_decay_rate = weight_decay_rate\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "    self.epsilon = epsilon\n",
        "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "\n",
        "  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    assignments = []\n",
        "    for (grad, param) in grads_and_vars:\n",
        "      if grad is None or param is None:\n",
        "        continue\n",
        "\n",
        "      param_name = self._get_variable_name(param.name)\n",
        "\n",
        "      m = tf.get_variable(\n",
        "          name=param_name + \"/adam_m\",\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "      v = tf.get_variable(\n",
        "          name=param_name + \"/adam_v\",\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "\n",
        "      # Standard Adam update.\n",
        "      next_m = (\n",
        "          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n",
        "      next_v = (\n",
        "          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n",
        "                                                    tf.square(grad)))\n",
        "\n",
        "      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n",
        "\n",
        "      # Just adding the square of the weights to the loss function is *not*\n",
        "      # the correct way of using L2 regularization/weight decay with Adam,\n",
        "      # since that will interact with the m and v parameters in strange ways.\n",
        "      #\n",
        "      # Instead we want ot decay the weights in a manner that doesn't interact\n",
        "      # with the m/v parameters. This is equivalent to adding the square\n",
        "      # of the weights to the loss with plain (non-momentum) SGD.\n",
        "      if self._do_use_weight_decay(param_name):\n",
        "        update += self.weight_decay_rate * param\n",
        "\n",
        "      update_with_lr = self.learning_rate * update\n",
        "\n",
        "      next_param = param - update_with_lr\n",
        "\n",
        "      assignments.extend(\n",
        "          [param.assign(next_param),\n",
        "           m.assign(next_m),\n",
        "           v.assign(next_v)])\n",
        "    return tf.group(*assignments, name=name)\n",
        "\n",
        "  def _do_use_weight_decay(self, param_name):\n",
        "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
        "    if not self.weight_decay_rate:\n",
        "      return False\n",
        "    if self.exclude_from_weight_decay:\n",
        "      for r in self.exclude_from_weight_decay:\n",
        "        if re.search(r, param_name) is not None:\n",
        "          return False\n",
        "    return True\n",
        "\n",
        "  def _get_variable_name(self, param_name):\n",
        "    \"\"\"Get the variable name from the tensor name.\"\"\"\n",
        "    m = re.match(\"^(.*):\\\\d+$\", param_name)\n",
        "    if m is not None:\n",
        "      param_name = m.group(1)\n",
        "    return param_name"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z0_jlypYe1xC"
      },
      "cell_type": "markdown",
      "source": [
        "#### Utils"
      ]
    },
    {
      "metadata": {
        "id": "viPxpeHGe1xD"
      },
      "cell_type": "code",
      "source": [
        "def str_to_value(input_str):\n",
        "    \"\"\"\n",
        "    Convert data type of value of dict to appropriate one.\n",
        "    Assume there are only three types: str, int, float.\n",
        "    \"\"\"\n",
        "    if input_str.isalpha():\n",
        "        return input_str\n",
        "    elif input_str.isdigit():\n",
        "        return int(input_str)\n",
        "    else:\n",
        "        return float(input_str)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_OdynM84e1xJ"
      },
      "cell_type": "markdown",
      "source": [
        "#### Sentence piece tokenization"
      ]
    },
    {
      "metadata": {
        "id": "vCjLnBdoe1xK"
      },
      "cell_type": "code",
      "source": [
        "def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n",
        "    \"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"\n",
        "\n",
        "    # The casing has to be passed in by the user and there is no explicit check\n",
        "    # as to whether it matches the checkpoint. The casing information probably\n",
        "    # should have been stored in the bert_config.json file, but it's not, so\n",
        "    # we have to heuristically detect it to validate.\n",
        "\n",
        "    if not init_checkpoint:\n",
        "        return\n",
        "\n",
        "    m = re.match(\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\", init_checkpoint)\n",
        "    if m is None:\n",
        "        return\n",
        "\n",
        "    model_name = m.group(1)\n",
        "\n",
        "    lower_models = [\n",
        "        \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n",
        "        \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n",
        "    ]\n",
        "\n",
        "    cased_models = [\n",
        "        \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n",
        "        \"multi_cased_L-12_H-768_A-12\"\n",
        "    ]\n",
        "\n",
        "    is_bad_config = False\n",
        "    if model_name in lower_models and not do_lower_case:\n",
        "        is_bad_config = True\n",
        "        actual_flag = \"False\"\n",
        "        case_name = \"lowercased\"\n",
        "        opposite_flag = \"True\"\n",
        "\n",
        "    if model_name in cased_models and do_lower_case:\n",
        "        is_bad_config = True\n",
        "        actual_flag = \"True\"\n",
        "        case_name = \"cased\"\n",
        "        opposite_flag = \"False\"\n",
        "\n",
        "    if is_bad_config:\n",
        "        raise ValueError(\n",
        "            \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n",
        "            \"However, `%s` seems to be a %s model, so you \"\n",
        "            \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n",
        "            \"how the model was pre-training. If this error is wrong, please \"\n",
        "            \"just comment out this check.\" % (actual_flag, init_checkpoint,\n",
        "                                              model_name, case_name, opposite_flag))\n",
        "\n",
        "\n",
        "def convert_to_unicode(text):\n",
        "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
        "    if six.PY3:\n",
        "        if isinstance(text, str):\n",
        "            return text\n",
        "        elif isinstance(text, bytes):\n",
        "            return text.decode(\"utf-8\", \"ignore\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "    elif six.PY2:\n",
        "        if isinstance(text, str):\n",
        "            return text.decode(\"utf-8\", \"ignore\")\n",
        "        elif isinstance(text, unicode):\n",
        "            return text\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "    else:\n",
        "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def printable_text(text):\n",
        "    \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
        "\n",
        "    # These functions want `str` for both Python2 and Python3, but in one case\n",
        "    # it's a Unicode string and in the other it's a byte string.\n",
        "    if six.PY3:\n",
        "        if isinstance(text, str):\n",
        "            return text\n",
        "        elif isinstance(text, bytes):\n",
        "            return text.decode(\"utf-8\", \"ignore\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "    elif six.PY2:\n",
        "        if isinstance(text, str):\n",
        "            return text\n",
        "        elif isinstance(text, unicode):\n",
        "            return text.encode(\"utf-8\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "    else:\n",
        "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "    vocab = collections.OrderedDict()\n",
        "    index = 0\n",
        "    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n",
        "        while True:\n",
        "            token = convert_to_unicode(reader.readline())\n",
        "            if not token:\n",
        "                break\n",
        "            token, _ = token.split(\"\\t\")\n",
        "            token = token.strip()\n",
        "            vocab[token] = index\n",
        "            index += 1\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def convert_by_vocab(vocab, items, unk_info):\n",
        "    \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n",
        "    output = []\n",
        "    for item in items:\n",
        "        if item in vocab:\n",
        "            output.append(vocab[item])\n",
        "        else:\n",
        "            output.append(unk_info)\n",
        "    return output\n",
        "\n",
        "\n",
        "def convert_tokens_to_ids(vocab, tokens):\n",
        "    \"\"\"Id of <unk> is assumed as 0 accroding to sentencepiece\"\"\"\n",
        "    return convert_by_vocab(vocab, tokens, unk_info=0)\n",
        "\n",
        "\n",
        "def convert_ids_to_tokens(inv_vocab, ids):\n",
        "    \"\"\"Token of unknown word is assumed as <unk> according to sentencepiece\"\"\"\n",
        "    return convert_by_vocab(inv_vocab, ids, unk_info=\"<unk>\")\n",
        "\n",
        "\n",
        "class FullTokenizer(object):\n",
        "    \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
        "\n",
        "    def __init__(self, model_file, vocab_file, do_lower_case=True):\n",
        "        self.tokenizer = SentencePieceTokenizer(model_file, do_lower_case=do_lower_case)\n",
        "        self.vocab = load_vocab(vocab_file)\n",
        "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        split_tokens = self.tokenizer.tokenize(text)\n",
        "        return split_tokens\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        \"\"\"Id of <unk> is assumed as 0 accroding to sentencepiece\"\"\"\n",
        "        return convert_by_vocab(self.vocab, tokens, unk_info=0)\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "        \"\"\"Token of unknown word is assumed as <unk> according to sentencepiece\"\"\"\n",
        "        return convert_by_vocab(self.inv_vocab, ids, unk_info=\"<unk>\")\n",
        "\n",
        "\n",
        "class SentencePieceTokenizer(object):\n",
        "    \"\"\"Runs SentencePiece tokenization (from raw text to tokens list)\"\"\"\n",
        "\n",
        "    def __init__(self, model_file=None, do_lower_case=True):\n",
        "        \"\"\"Constructs a SentencePieceTokenizer.\"\"\"\n",
        "        self.tokenizer = sp.SentencePieceProcessor()\n",
        "        if self.tokenizer.Load(model_file):\n",
        "            print(\"Loaded a trained SentencePiece model.\")\n",
        "        else:\n",
        "            print(\"You have to give a path of trained SentencePiece model.\")\n",
        "            sys.exit(1)\n",
        "        self.do_lower_case = do_lower_case\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text.\"\"\"\n",
        "        text = convert_to_unicode(text)\n",
        "        if self.do_lower_case:\n",
        "            text = text.lower()\n",
        "        # Tokenization\n",
        "        output_tokens = self.tokenizer.EncodeAsPieces(text)\n",
        "        return output_tokens\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wq34WW-Ye1xO"
      },
      "cell_type": "markdown",
      "source": [
        "#### A method for running classifier"
      ]
    },
    {
      "metadata": {
        "id": "sqPOFUjie1xQ",
        "outputId": "f92a13dd-4e8e-4933-86ef-16073ed87076",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "CONFIGPATH = './config.ini'\n",
        "config = configparser.ConfigParser()\n",
        "config.read(CONFIGPATH)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./config.ini']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "oKL1bDpze1xg"
      },
      "cell_type": "code",
      "source": [
        "PRETRAINED_MODEL_PATH = u'./model/model.ckpt-1000000'\n",
        "FINETUNE_OUTPUT_DIR = u'./model/ntt_output'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QZPhHXPDe1xo"
      },
      "cell_type": "code",
      "source": [
        "CONFIGPATH = './config.ini'\n",
        "config = configparser.ConfigParser()\n",
        "config.read(CONFIGPATH)\n",
        "bert_config_file = tempfile.NamedTemporaryFile(mode='w+t', encoding='utf-8', suffix='.json')\n",
        "bert_config_file.write(json.dumps({k:str_to_value(v) for k,v in config['BERT-CONFIG'].items()}))\n",
        "bert_config_file.seek(0)\n",
        "\n",
        "\n",
        "class InputExample(object):\n",
        "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "  def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "    \"\"\"Constructs a InputExample.\n",
        "\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "    self.guid = guid\n",
        "    self.text_a = text_a\n",
        "    self.text_b = text_b\n",
        "    self.label = label\n",
        "\n",
        "\n",
        "class PaddingInputExample(object):\n",
        "  \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "\n",
        "  When running eval/predict on the TPU, we need to pad the number of examples\n",
        "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "  size. The alternative is to drop the last batch, which is bad because it means\n",
        "  the entire output data won't be generated.\n",
        "\n",
        "  We use this class instead of `None` because treating `None` as padding\n",
        "  battches could cause silent errors.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "  \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               input_ids,\n",
        "               input_mask,\n",
        "               segment_ids,\n",
        "               label_id,\n",
        "               is_real_example=True):\n",
        "    self.input_ids = input_ids\n",
        "    self.input_mask = input_mask\n",
        "    self.segment_ids = segment_ids\n",
        "    self.label_id = label_id\n",
        "    self.is_real_example = is_real_example\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "  \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "  def get_train_examples(self, data_dir):\n",
        "    \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def get_dev_examples(self, data_dir):\n",
        "    \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def get_test_examples(self, data_dir):\n",
        "    \"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def get_labels(self):\n",
        "    \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  @classmethod\n",
        "  def _read_tsv(cls, input_file, quotechar=None):\n",
        "    \"\"\"Reads a tab separated value file.\"\"\"\n",
        "    with tf.gfile.Open(input_file, \"r\") as f:\n",
        "      reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "      lines = []\n",
        "      for line in reader:\n",
        "        lines.append(line)\n",
        "      return lines\n",
        "\n",
        "    \n",
        "# 本講義のためのメソッド.\n",
        "class NTTProcessor(DataProcessor):\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return list(range(24))\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_examples(lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if i == 0:\n",
        "                idx_text = line.index('query')\n",
        "                idx_label = line.index('label')\n",
        "            else:\n",
        "                guid = \"%s-%s\" % (set_type, i)\n",
        "                text_a = convert_to_unicode(line[idx_text])\n",
        "                label = convert_to_unicode(line[idx_label])\n",
        "                examples.append(\n",
        "                    InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "      \n",
        "\n",
        "def convert_single_example(ex_index, example, label_list, max_seq_length,\n",
        "                           tokenizer):\n",
        "  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "\n",
        "  if isinstance(example, PaddingInputExample):\n",
        "    return InputFeatures(\n",
        "        input_ids=[0] * max_seq_length,\n",
        "        input_mask=[0] * max_seq_length,\n",
        "        segment_ids=[0] * max_seq_length,\n",
        "        label_id=0,\n",
        "        is_real_example=False)\n",
        "\n",
        "  label_map = {}\n",
        "  for (i, label) in enumerate(label_list):\n",
        "    label_map[str(label)] = i\n",
        "  tokens_a = tokenizer.tokenize(example.text_a)\n",
        "  tokens_b = None\n",
        "  if example.text_b:\n",
        "    tokens_b = tokenizer.tokenize(example.text_b)\n",
        "\n",
        "  if tokens_b:\n",
        "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "    # length is less than the specified length.\n",
        "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "  else:\n",
        "    # Account for [CLS] and [SEP] with \"- 2\"\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "      tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
        "\n",
        "  # The convention in BERT is:\n",
        "  # (a) For sequence pairs:\n",
        "  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "  # (b) For single sequences:\n",
        "  #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "  #  type_ids: 0     0   0   0  0     0 0\n",
        "  #\n",
        "  # Where \"type_ids\" are used to indicate whether this is the first\n",
        "  # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "  # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "  # embedding vector (and position vector). This is not *strictly* necessary\n",
        "  # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "  # it easier for the model to learn the concept of sequences.\n",
        "  #\n",
        "  # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "  # used as the \"sentence vector\". Note that this only makes sense because\n",
        "  # the entire model is fine-tuned.\n",
        "  tokens = []\n",
        "  segment_ids = []\n",
        "  tokens.append(\"[CLS]\")\n",
        "  segment_ids.append(0)\n",
        "  for token in tokens_a:\n",
        "    tokens.append(token)\n",
        "    segment_ids.append(0)\n",
        "  tokens.append(\"[SEP]\")\n",
        "  segment_ids.append(0)\n",
        "\n",
        "  if tokens_b:\n",
        "    for token in tokens_b:\n",
        "      tokens.append(token)\n",
        "      segment_ids.append(1)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(1)\n",
        "\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "  # tokens are attended to.\n",
        "  input_mask = [1] * len(input_ids)\n",
        "\n",
        "  # Zero-pad up to the sequence length.\n",
        "  while len(input_ids) < max_seq_length:\n",
        "    input_ids.append(0)\n",
        "    input_mask.append(0)\n",
        "    segment_ids.append(0)\n",
        "\n",
        "  assert len(input_ids) == max_seq_length\n",
        "  assert len(input_mask) == max_seq_length\n",
        "  assert len(segment_ids) == max_seq_length\n",
        "\n",
        "  label_id = label_map[example.label]\n",
        "  if ex_index < 5:\n",
        "    tf.logging.info(\"*** Example ***\")\n",
        "    tf.logging.info(\"guid: %s\" % (example.guid))\n",
        "    tf.logging.info(\"tokens: %s\" % \" \".join(\n",
        "        [printable_text(x) for x in tokens]))\n",
        "    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "    tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
        "\n",
        "  feature = InputFeatures(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids,\n",
        "      label_id=label_id,\n",
        "      is_real_example=True)\n",
        "  return feature\n",
        "\n",
        "\n",
        "def file_based_convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, output_file):\n",
        "  \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
        "\n",
        "  writer = tf.python_io.TFRecordWriter(output_file)\n",
        "\n",
        "  for (ex_index, example) in enumerate(examples):\n",
        "    if ex_index % 10000 == 0:\n",
        "      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "    feature = convert_single_example(ex_index, example, label_list,\n",
        "                                     max_seq_length, tokenizer)\n",
        "\n",
        "    def create_int_feature(values):\n",
        "      f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
        "      return f\n",
        "\n",
        "    features = collections.OrderedDict()\n",
        "    features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
        "    features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
        "    features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
        "    features[\"label_ids\"] = create_int_feature([feature.label_id])\n",
        "    features[\"is_real_example\"] = create_int_feature(\n",
        "        [int(feature.is_real_example)])\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "    writer.write(tf_example.SerializeToString())\n",
        "  writer.close()\n",
        "\n",
        "\n",
        "def file_based_input_fn_builder(input_file, seq_length, is_training,\n",
        "                                drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  name_to_features = {\n",
        "      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"label_ids\": tf.FixedLenFeature([], tf.int64),\n",
        "      \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
        "  }\n",
        "\n",
        "  def _decode_record(record, name_to_features):\n",
        "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "    example = tf.parse_single_example(record, name_to_features)\n",
        "\n",
        "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "    # So cast all int64 to int32.\n",
        "    for name in list(example.keys()):\n",
        "      t = example[name]\n",
        "      if t.dtype == tf.int64:\n",
        "        t = tf.to_int32(t)\n",
        "      example[name] = t\n",
        "\n",
        "    return example\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    # For training, we want a lot of parallel reading and shuffling.\n",
        "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
        "    d = tf.data.TFRecordDataset(input_file)\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.apply(\n",
        "        tf.contrib.data.map_and_batch(\n",
        "            lambda record: _decode_record(record, name_to_features),\n",
        "            batch_size=batch_size,\n",
        "            drop_remainder=drop_remainder))\n",
        "\n",
        "    return d\n",
        "\n",
        "  return input_fn\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "  # This is a simple heuristic which will always truncate the longer sequence\n",
        "  # one token at a time. This makes more sense than truncating an equal percent\n",
        "  # of tokens from each, since if one sequence is very short then each token\n",
        "  # that's truncated likely contains more information than a longer sequence.\n",
        "  while True:\n",
        "    total_length = len(tokens_a) + len(tokens_b)\n",
        "    if total_length <= max_length:\n",
        "      break\n",
        "    if len(tokens_a) > len(tokens_b):\n",
        "      tokens_a.pop()\n",
        "    else:\n",
        "      tokens_b.pop()\n",
        "\n",
        "\n",
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels, use_one_hot_embeddings):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  # In the demo, we are doing a simple classification task on the entire\n",
        "  # segment.\n",
        "  #\n",
        "  # If you want to use the token-level output, use model.get_sequence_output()\n",
        "  # instead.\n",
        "  output_layer = model.get_pooled_output()\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, logits, probabilities)\n",
        "\n",
        "\n",
        "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "    is_real_example = None\n",
        "    if \"is_real_example\" in features:\n",
        "      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
        "    else:\n",
        "      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (total_loss, per_example_loss, logits, probabilities) = create_model(\n",
        "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "        num_labels, use_one_hot_embeddings)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "       ) = get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "      train_op = create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op,\n",
        "          scaffold_fn=scaffold_fn)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
        "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(\n",
        "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
        "        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"eval_loss\": loss,\n",
        "        }\n",
        "\n",
        "      eval_metrics = (metric_fn,\n",
        "                      [per_example_loss, label_ids, logits, is_real_example])\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          eval_metrics=eval_metrics,\n",
        "          scaffold_fn=scaffold_fn)\n",
        "    else:\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          predictions={\"probabilities\": probabilities},\n",
        "          scaffold_fn=scaffold_fn)\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn\n",
        "\n",
        "\n",
        "# This function is not used by this file but is still used by the Colab and\n",
        "# people who depend on it.\n",
        "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_id)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "\n",
        "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"label_ids\":\n",
        "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "\n",
        "  return input_fn\n",
        "\n",
        "\n",
        "# This function is not used by this file but is still used by the Colab and\n",
        "# people who depend on it.\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "  \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "\n",
        "  features = []\n",
        "  for (ex_index, example) in enumerate(examples):\n",
        "    if ex_index % 10000 == 0:\n",
        "      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "    feature = convert_single_example(ex_index, example, label_list,\n",
        "                                     max_seq_length, tokenizer)\n",
        "\n",
        "    features.append(feature)\n",
        "  return features\n",
        "\n",
        "\n",
        "def main():\n",
        "  # Fine-tuningを行うためのハイパーパラメタ、その他情報を登録\n",
        "  task_name = 'ntt'\n",
        "  do_train = True\n",
        "  do_eval = True\n",
        "  do_predict = False\n",
        "  data_dir = './data/ntt'\n",
        "  model_file = './model/wiki-ja.model'\n",
        "  vocab_file = './model/wiki-ja.vocab'\n",
        "  init_checkpoint = PRETRAINED_MODEL_PATH\n",
        "  do_lower_case = True\n",
        "  max_seq_length=32\n",
        "  train_batch_size=16\n",
        "  learning_rate=5e-5\n",
        "  num_train_epochs=3.0\n",
        "  output_dir= FINETUNE_OUTPUT_DIR\n",
        "  train_batch_size = 32\n",
        "  eval_batch_size = 8\n",
        "  predict_batch_size = 8\n",
        "  warmup_proportion = 0.1\n",
        "  save_checkpoints_steps = 1000\n",
        "  iterations_per_loop = 1000\n",
        "  use_tpu = False\n",
        "  tpu_name = None\n",
        "  tpu_zone = None\n",
        "  gcp_project = None\n",
        "  master = None\n",
        "  num_tpu_cores = 8\n",
        "  ### end\n",
        "    \n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "  processors = {\n",
        "      \"ntt\": NTTProcessor,\n",
        "  }\n",
        "\n",
        "  validate_case_matches_checkpoint(do_lower_case,\n",
        "                                                init_checkpoint)\n",
        "\n",
        "  if not do_train and not do_eval and not do_predict:\n",
        "    raise ValueError(\n",
        "        \"At least one of `do_train`, `do_eval` or `do_predict' must be True.\")\n",
        "\n",
        "  bert_config = BertConfig.from_json_file(bert_config_file.name)\n",
        "\n",
        "  if max_seq_length > bert_config.max_position_embeddings:\n",
        "    raise ValueError(\n",
        "        \"Cannot use sequence length %d because the BERT model \"\n",
        "        \"was only trained up to sequence length %d\" %\n",
        "        (max_seq_length, bert_config.max_position_embeddings))\n",
        "\n",
        "  tf.gfile.MakeDirs(output_dir)\n",
        "\n",
        "  task_name = task_name.lower()\n",
        "\n",
        "  if task_name not in processors:\n",
        "    raise ValueError(\"Task not found: %s\" % (task_name))\n",
        "\n",
        "  processor = processors[task_name]()\n",
        "\n",
        "  label_list = processor.get_labels()\n",
        "\n",
        "  tokenizer = FullTokenizer(\n",
        "      model_file=model_file, vocab_file=vocab_file,\n",
        "      do_lower_case=do_lower_case)\n",
        "\n",
        "  tpu_cluster_resolver = None\n",
        "  if use_tpu and tpu_name:\n",
        "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
        "        tpu_name, zone=tpu_zone, project=gcp_project)\n",
        "\n",
        "  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "  run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      master=master,\n",
        "      model_dir=output_dir,\n",
        "      save_checkpoints_steps=save_checkpoints_steps,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          iterations_per_loop=iterations_per_loop,\n",
        "          num_shards=num_tpu_cores,\n",
        "          per_host_input_for_training=is_per_host))\n",
        "\n",
        "  train_examples = None\n",
        "  num_train_steps = None\n",
        "  num_warmup_steps = None\n",
        "  if do_train:\n",
        "    train_examples = processor.get_train_examples(data_dir)\n",
        "    num_train_steps = int(\n",
        "        len(train_examples) / train_batch_size * num_train_epochs)\n",
        "    num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "  model_fn = model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      num_labels=len(label_list),\n",
        "      init_checkpoint=init_checkpoint,\n",
        "      learning_rate=learning_rate,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      use_tpu=use_tpu,\n",
        "      use_one_hot_embeddings=use_tpu)\n",
        "\n",
        "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "  # or GPU.\n",
        "  estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=use_tpu,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=train_batch_size,\n",
        "      eval_batch_size=eval_batch_size,\n",
        "      predict_batch_size=predict_batch_size)\n",
        "\n",
        "  if do_train:\n",
        "    train_file = os.path.join(output_dir, \"train.tf_record\")\n",
        "    file_based_convert_examples_to_features(\n",
        "        train_examples, label_list, max_seq_length, tokenizer, train_file)\n",
        "    tf.logging.info(\"***** Running training *****\")\n",
        "    tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
        "    tf.logging.info(\"  Batch size = %d\", train_batch_size)\n",
        "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "    train_input_fn = file_based_input_fn_builder(\n",
        "        input_file=train_file,\n",
        "        seq_length=max_seq_length,\n",
        "        is_training=True,\n",
        "        drop_remainder=True)\n",
        "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "\n",
        "  if do_eval:\n",
        "    eval_examples = processor.get_dev_examples(data_dir)\n",
        "    num_actual_eval_examples = len(eval_examples)\n",
        "    if use_tpu:\n",
        "      # TPU requires a fixed batch size for all batches, therefore the number\n",
        "      # of examples must be a multiple of the batch size, or else examples\n",
        "      # will get dropped. So we pad with fake examples which are ignored\n",
        "      # later on. These do NOT count towards the metric (all tf.metrics\n",
        "      # support a per-instance weight, and these get a weight of 0.0).\n",
        "      while len(eval_examples) % eval_batch_size != 0:\n",
        "        eval_examples.append(PaddingInputExample())\n",
        "\n",
        "    eval_file = os.path.join(output_dir, \"eval.tf_record\")\n",
        "    file_based_convert_examples_to_features(\n",
        "        eval_examples, label_list, max_seq_length, tokenizer, eval_file)\n",
        "\n",
        "    tf.logging.info(\"***** Running evaluation *****\")\n",
        "    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
        "                    len(eval_examples), num_actual_eval_examples,\n",
        "                    len(eval_examples) - num_actual_eval_examples)\n",
        "    tf.logging.info(\"  Batch size = %d\", eval_batch_size)\n",
        "\n",
        "    # This tells the estimator to run through the entire set.\n",
        "    eval_steps = None\n",
        "    # However, if running eval on the TPU, you will need to specify the\n",
        "    # number of steps.\n",
        "    if use_tpu:\n",
        "      assert len(eval_examples) % eval_batch_size == 0\n",
        "      eval_steps = int(len(eval_examples) // eval_batch_size)\n",
        "\n",
        "    eval_drop_remainder = True if use_tpu else False\n",
        "    eval_input_fn = file_based_input_fn_builder(\n",
        "        input_file=eval_file,\n",
        "        seq_length=max_seq_length,\n",
        "        is_training=False,\n",
        "        drop_remainder=eval_drop_remainder)\n",
        "\n",
        "    result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "\n",
        "    output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
        "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "      tf.logging.info(\"***** Eval results *****\")\n",
        "      for key in sorted(result.keys()):\n",
        "        tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
        "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "  if do_predict:\n",
        "    predict_examples = processor.get_test_examples(data_dir)\n",
        "    num_actual_predict_examples = len(predict_examples)\n",
        "    if use_tpu:\n",
        "      # TPU requires a fixed batch size for all batches, therefore the number\n",
        "      # of examples must be a multiple of the batch size, or else examples\n",
        "      # will get dropped. So we pad with fake examples which are ignored\n",
        "      # later on.\n",
        "      while len(predict_examples) % predict_batch_size != 0:\n",
        "        predict_examples.append(PaddingInputExample())\n",
        "\n",
        "    predict_file = os.path.join(output_dir, \"predict.tf_record\")\n",
        "    file_based_convert_examples_to_features(predict_examples, label_list,\n",
        "                                            max_seq_length, tokenizer,\n",
        "                                            predict_file)\n",
        "\n",
        "    tf.logging.info(\"***** Running prediction*****\")\n",
        "    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
        "                    len(predict_examples), num_actual_predict_examples,\n",
        "                    len(predict_examples) - num_actual_predict_examples)\n",
        "    tf.logging.info(\"  Batch size = %d\", predict_batch_size)\n",
        "\n",
        "    predict_drop_remainder = True if use_tpu else False\n",
        "    predict_input_fn = file_based_input_fn_builder(\n",
        "        input_file=predict_file,\n",
        "        seq_length=max_seq_length,\n",
        "        is_training=False,\n",
        "        drop_remainder=predict_drop_remainder)\n",
        "\n",
        "    result = estimator.predict(input_fn=predict_input_fn)\n",
        "\n",
        "    output_predict_file = os.path.join(output_dir, \"test_results.tsv\")\n",
        "    with tf.gfile.GFile(output_predict_file, \"w\") as writer:\n",
        "      num_written_lines = 0\n",
        "      tf.logging.info(\"***** Predict results *****\")\n",
        "      for (i, prediction) in enumerate(result):\n",
        "        probabilities = prediction[\"probabilities\"]\n",
        "        if i >= num_actual_predict_examples:\n",
        "          break\n",
        "        output_line = \"\\t\".join(\n",
        "            str(class_probability)\n",
        "            for class_probability in probabilities) + \"\\n\"\n",
        "        writer.write(output_line)\n",
        "        num_written_lines += 1\n",
        "    assert num_written_lines == num_actual_predict_examples"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SqGmSzSMe1xq",
        "outputId": "e327e5bf-5015-450a-9625-3ad0bb2fb4e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-26429d0c0eab>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    559\u001b[0m   \u001b[0;31m### end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m   processors = {\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'logging'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MKOPtue5dBm9"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": 12,
      "outputs": []
    }
  ]
}