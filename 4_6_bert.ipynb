{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ffalo64/rabbit_challenge/blob/main/4_6_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnyQzhRRLCZK",
        "outputId": "5a333e23-f399-4911-d6ee-ed041ee8e076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.10/dist-packages (1.0.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unidic in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: plac<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from unidic) (1.3.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from unidic) (2.27.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from unidic) (4.65.0)\n",
            "Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from unidic) (0.10.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->unidic) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->unidic) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->unidic) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->unidic) (3.4)\n",
            "download url: https://cotonoha-dic.s3-ap-northeast-1.amazonaws.com/unidic-3.1.0.zip\n",
            "Dictionary version: 3.1.0+2021-08-31\n",
            "Downloading UniDic v3.1.0+2021-08-31...\n",
            "unidic-3.1.0.zip: 100% 526M/526M [00:17<00:00, 29.8MB/s]\n",
            "Finished download.\n",
            "Downloaded UniDic v3.1.0+2021-08-31 to /usr/local/lib/python3.10/dist-packages/unidic/dicdir\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fugashi in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ipadic in /usr/local/lib/python3.10/dist-packages (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install mecab-python3\n",
        "!pip install unidic\n",
        "!python -m unidic download\n",
        "!pip install fugashi\n",
        "!pip install ipadic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24oJiQ8PNm6X",
        "outputId": "ebae63a2-ddbf-4232-9b43-09b36b27693f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc_MRRv6_CcF"
      },
      "source": [
        "青空文庫から夏目漱石の\n",
        "「それから」\n",
        "「こころ」\n",
        "「夢十夜」\n",
        "をダウンロードしてくる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycNXyyaxOaFH",
        "outputId": "6ccffca8-1ac0-4199-c7a7-6d6fea3466da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-04 09:03:12--  https://www.aozora.gr.jp/cards/000148/files/773_ruby_5968.zip\n",
            "Resolving www.aozora.gr.jp (www.aozora.gr.jp)... 59.106.13.115\n",
            "Connecting to www.aozora.gr.jp (www.aozora.gr.jp)|59.106.13.115|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 153688 (150K) [application/zip]\n",
            "Saving to: ‘773_ruby_5968.zip.1’\n",
            "\n",
            "773_ruby_5968.zip.1 100%[===================>] 150.09K   339KB/s    in 0.4s    \n",
            "\n",
            "2023-05-04 09:03:13 (339 KB/s) - ‘773_ruby_5968.zip.1’ saved [153688/153688]\n",
            "\n",
            "Archive:  /content/773_ruby_5968.zip\n",
            "Made with MacWinZipper™\n",
            "replace kokoro.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: kokoro.txt              \n",
            "--2023-05-04 09:03:20--  https://www.aozora.gr.jp/cards/000148/files/56143_ruby_50824.zip\n",
            "Resolving www.aozora.gr.jp (www.aozora.gr.jp)... 59.106.13.115\n",
            "Connecting to www.aozora.gr.jp (www.aozora.gr.jp)|59.106.13.115|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 176731 (173K) [application/zip]\n",
            "Saving to: ‘56143_ruby_50824.zip.1’\n",
            "\n",
            "56143_ruby_50824.zi 100%[===================>] 172.59K   321KB/s    in 0.5s    \n",
            "\n",
            "2023-05-04 09:03:21 (321 KB/s) - ‘56143_ruby_50824.zip.1’ saved [176731/176731]\n",
            "\n",
            "Archive:  /content/56143_ruby_50824.zip\n",
            "Made with MacWinZipper™\n",
            "replace sorekara.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: sorekara.txt            \n",
            "--2023-05-04 09:03:26--  https://www.aozora.gr.jp/cards/000148/files/799_ruby_6024.zip\n",
            "Resolving www.aozora.gr.jp (www.aozora.gr.jp)... 59.106.13.115\n",
            "Connecting to www.aozora.gr.jp (www.aozora.gr.jp)|59.106.13.115|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18936 (18K) [application/zip]\n",
            "Saving to: ‘799_ruby_6024.zip.1’\n",
            "\n",
            "799_ruby_6024.zip.1 100%[===================>]  18.49K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-05-04 09:03:27 (209 KB/s) - ‘799_ruby_6024.zip.1’ saved [18936/18936]\n",
            "\n",
            "Archive:  799_ruby_6024.zip\n",
            "Made with MacWinZipper™\n",
            "replace yume_juya.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: yume_juya.txt           \n"
          ]
        }
      ],
      "source": [
        "!wget https://www.aozora.gr.jp/cards/000148/files/773_ruby_5968.zip\n",
        "!unzip -O sjjs /content/773_ruby_5968.zip\n",
        "!wget https://www.aozora.gr.jp/cards/000148/files/56143_ruby_50824.zip\n",
        "!unzip -O sjjs  /content/56143_ruby_50824.zip\n",
        "!wget https://www.aozora.gr.jp/cards/000148/files/799_ruby_6024.zip\n",
        "!unzip -O sjjs 799_ruby_6024.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvIa4940dGPX",
        "outputId": "0e06a602-bd07-4e68-bee1-232c7ddc955c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "nkf is already the newest version (1:2.1.5-1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt install nkf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SAWKmgnrdP1s"
      },
      "outputs": [],
      "source": [
        "!nkf -w --overwrite kokoro.txt sorekara.txt yume_juya.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GDVIiOAMwiaz"
      },
      "outputs": [],
      "source": [
        "!cat kokoro.txt sorekara.txt yume_juya.txt > train.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZlAgWfINnh4",
        "outputId": "3c82a1ee-c148-49e8-f9a0-4269d455e181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFBertModel\n",
        "from transformers import BertJapaneseTokenizer\n",
        "\n",
        "\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
        "\n",
        "bert = TFBertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OS5NcVosMcv3"
      },
      "outputs": [],
      "source": [
        "import MeCab\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dlHfGEMfvzjE"
      },
      "outputs": [],
      "source": [
        "with open('train.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read().replace('\\n', '')\n",
        "mecab = MeCab.Tagger(\"-Owakati\")\n",
        "text = mecab.parse(text).split()\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2LadQxfjxZJK"
      },
      "outputs": [],
      "source": [
        "seq_length = 128\n",
        "\n",
        "# 訓練用サンプルとターゲットを作る\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgXj3CNpphYp",
        "outputId": "b7c3eda0-e54d-4aaf-f5cf-c83ff4b85120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data: 'こころ夏目漱石-------------------------------------------------------【テキスト中に現れる記号について】《》：ルビ（例）私《わたくし》は｜：ルビの付く文字列の始まりを特定する記号（例）先生一人｜麦藁帽《むぎわらぼう》を［＃］：入力者注主に外字の説明や、傍点の位置の指定（数字は、JISX0213の面区点番号、または底本のページと行数）（例）※［＃「てへん'\n",
            "Target data: '夏目漱石-------------------------------------------------------【テキスト中に現れる記号について】《》：ルビ（例）私《わたくし》は｜：ルビの付く文字列の始まりを特定する記号（例）先生一人｜麦藁帽《むぎわらぼう》を［＃］：入力者注主に外字の説明や、傍点の位置の指定（数字は、JISX0213の面区点番号、または底本のページと行数）（例）※［＃「てへん＋'\n",
            "Input data: '劣」、第3水準1-84-77］-------------------------------------------------------［＃２字下げ］上先生と私［＃「上先生と私」は大見出し］［＃５字下げ］一［＃「一」は中見出し］私《わたくし》はその人を常に先生と呼んでいた。だからここでもただ先生と書くだけで本名は打ち明けない。これは世間を憚《はば》かる遠慮というよりも、その'\n",
            "Target data: '」、第3水準1-84-77］-------------------------------------------------------［＃２字下げ］上先生と私［＃「上先生と私」は大見出し］［＃５字下げ］一［＃「一」は中見出し］私《わたくし》はその人を常に先生と呼んでいた。だからここでもただ先生と書くだけで本名は打ち明けない。これは世間を憚《はば》かる遠慮というよりも、その方'\n",
            "Input data: 'が私にとって自然だからである。私はその人の記憶を呼び起すごとに、すぐ「先生」といいたくなる。筆を執《と》っても心持は同じ事である。よそよそしい頭文字《かしらもじ》などはとても使う気にならない。私が先生と知り合いになったのは鎌倉《かまくら》である。その時私はまだ若々しい書生であった。暑中休暇を利用して海水浴に行った友達からぜひ来いという端書《はがき》を受け取ったので、私は多少の金を工面《くめん》し'\n",
            "Target data: '私にとって自然だからである。私はその人の記憶を呼び起すごとに、すぐ「先生」といいたくなる。筆を執《と》っても心持は同じ事である。よそよそしい頭文字《かしらもじ》などはとても使う気にならない。私が先生と知り合いになったのは鎌倉《かまくら》である。その時私はまだ若々しい書生であった。暑中休暇を利用して海水浴に行った友達からぜひ来いという端書《はがき》を受け取ったので、私は多少の金を工面《くめん》して'\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(3):\n",
        "    print(f'Input data: {repr(\"\".join(idx2char[input_example.numpy()]))}')\n",
        "    print(f'Target data: {repr(\"\".join(idx2char[target_example.numpy()]))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP8h98aFTjOS"
      },
      "source": [
        "ラベルのサイズ(バッチサイズ、　文の長さ)\n",
        "\n",
        "出力のサイズ（バッチサイズ、　文の長さ、　ボキャブラリーサイズ）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THKXvUiAd-sz",
        "outputId": "de951089-c548-4543-d106-f2ee95cc1857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "33/33 [==============================] - 1403s 42s/step - loss: 9.4696\n",
            "Epoch 2/5\n",
            "33/33 [==============================] - 1369s 41s/step - loss: 9.4684\n",
            "Epoch 3/5\n",
            "33/33 [==============================] - 1365s 41s/step - loss: 9.4679\n",
            "Epoch 4/5\n",
            "33/33 [==============================] - 1348s 41s/step - loss: 9.4672\n",
            "Epoch 5/5\n",
            "33/33 [==============================] - 1378s 42s/step - loss: 9.4663\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb8d3ed4250>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "input_ids = tf.keras.layers.Input(shape=(None, ), dtype='int32', name='input_ids')\n",
        "inputs = [input_ids]\n",
        "\n",
        "bert.trainable = False\n",
        "x = bert(inputs)\n",
        "\n",
        "out = x[0]\n",
        "\n",
        "Y = tf.keras.layers.Dense(len(vocab))(out)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=Y)\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-7))\n",
        "\n",
        "model.fit(dataset,epochs=5, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XiUp6xVb1f9x"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_string):\n",
        "  # 評価ステップ（学習済みモデルを使ったテキスト生成）\n",
        "\n",
        "  # 生成する文字数\n",
        "  num_generate = 30\n",
        "\n",
        "  # 開始文字列を数値に変換（ベクトル化）\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # 結果を保存する空文字列\n",
        "  text_generated = []\n",
        "\n",
        "  # 低い temperature　は、より予測しやすいテキストをもたらし\n",
        "  # 高い temperature は、より意外なテキストをもたらす\n",
        "  # 実験により最適な設定を見つけること\n",
        "  temperature = 1\n",
        "\n",
        "  # ここではバッチサイズ　== 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # バッチの次元を削除\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # カテゴリー分布をつかってモデルから返された言葉を予測 \n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # 過去の隠れ状態とともに予測された言葉をモデルへのつぎの入力として渡す\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (''.join(start_string) + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "y8kz6emY9erh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a78b84bf-42a9-4f59-e184-e0303e51ac25"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'私はたいまつ頻文士散らつきあらしきせる緑色打撃ぼんやり疑い深い資産濡らす寝ようかいだん昔やり過ごしうったえ堪羊思い出そううのばくろうとい金魚斬り付けおぼつかふん鼻頭出はいり日本橋'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "text = '私は'\n",
        "mecab = MeCab.Tagger(\"-Owakati\")\n",
        "text = mecab.parse(text).split()\n",
        "generate_text(model, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "AoNYqcNwvdg7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1n1haJHIjkMC"
      },
      "outputs": [],
      "source": [
        "def input_target(chunk):\n",
        "    input_text = chunk\n",
        "    target = tf.constant([1, 0, 0], dtype=tf.float32)\n",
        "    return input_text, target\n",
        "\n",
        "kokoro = tf.data.TextLineDataset('kokoro.txt')\n",
        "kokoro = kokoro.map(input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SfiNaxf_kbdm"
      },
      "outputs": [],
      "source": [
        "def input_target(chunk):\n",
        "    input_text = chunk\n",
        "    target = tf.constant([0, 1, 0], dtype=tf.float32)\n",
        "    return input_text, target\n",
        "\n",
        "sorekara = tf.data.TextLineDataset('sorekara.txt')\n",
        "sorekara = sorekara.map(input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Y7rha8H8kuvI"
      },
      "outputs": [],
      "source": [
        "def input_target(chunk):\n",
        "    input_text = chunk\n",
        "    target = tf.constant([0, 0, 1], dtype=tf.float32)\n",
        "    return input_text, target\n",
        "\n",
        "yume_juya = tf.data.TextLineDataset('yume_juya.txt')\n",
        "yume_juya = yume_juya.map(input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "GYpXS3aSk2Qd"
      },
      "outputs": [],
      "source": [
        "train_dataset = kokoro.concatenate(sorekara).concatenate(yume_juya)\n",
        "\n",
        "def tokenize_map_fn(tokenizer):\n",
        "\n",
        "    \"\"\"map function for pretrained tokenizer\"\"\"\n",
        "    def _tokenize(text_a, label):\n",
        "        inputs = tokenizer.encode_plus(\n",
        "            text_a.numpy().decode('utf-8'),\n",
        "            add_special_tokens=True,\n",
        "        )\n",
        "        input_ids= inputs[\"input_ids\"]\n",
        "        return input_ids, label\n",
        "\n",
        "    def _map_fn(text,label):\n",
        "        out = tf.py_function(_tokenize, inp=[text, label], Tout=(tf.int32, tf.float32))\n",
        "        return (out[0], out[1])\n",
        "\n",
        "    return _map_fn\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_map_fn(tokenizer))\n",
        "train_dataset = train_dataset.map(lambda x, y : (x[:128], y))\n",
        "train_dataset = train_dataset.padded_batch(64, padded_shapes=([128], [3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bza1i9kelF1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77813c60-638a-4106-faf0-f3a0408efa04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58/58 [==============================] - 1922s 33s/step - loss: 1.0676\n",
            "Epoch 2/5\n",
            "58/58 [==============================] - 1892s 33s/step - loss: 1.0646\n",
            "Epoch 3/5\n",
            "58/58 [==============================] - 1964s 34s/step - loss: 1.0642\n",
            "Epoch 4/5\n",
            "58/58 [==============================] - 1924s 33s/step - loss: 1.0607\n",
            "Epoch 5/5\n",
            "58/58 [==============================] - 1999s 34s/step - loss: 1.0587\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb8a8c3b640>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "\n",
        "input_ids = tf.keras.layers.Input(shape=(None, ), dtype='int32', name='input_ids')\n",
        "inputs = [input_ids]\n",
        "\n",
        "bert.trainable = False\n",
        "x = bert(inputs)\n",
        "\n",
        "out = x[1]\n",
        "\n",
        "fully_connected = tf.keras.layers.Dense(256, activation='relu')(out)\n",
        "Y = tf.keras.layers.Dense(3, activation='softmax')(fully_connected)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=Y)\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.categorical_crossentropy(labels, logits)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-7))\n",
        "\n",
        "model.fit(dataset,epochs=5, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "AiLa_AcnlNCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04319369-641e-4fe7-ccf7-d7c1a302f3ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.317373  , 0.3349539 , 0.34767312]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "\n",
        "text = '楽しい勉強でした。'\n",
        "\n",
        "\n",
        "encoded = tokenizer.encode_plus(\n",
        "            text,\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "inputs = tf.expand_dims(encoded[\"input_ids\"],0)\n",
        "res = model.predict_on_batch(inputs)\n",
        "res"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}